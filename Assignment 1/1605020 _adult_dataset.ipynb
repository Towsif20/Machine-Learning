{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1605020.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC1U04CHb9fh",
        "outputId": "830ed04e-26b9-4af6-f469-0726b1893676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/Adaboost/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xdFnWSQcv8h",
        "outputId": "d02f8717-de51-4099-c8d2-59cd7ef5cd13"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/MyDrive/Adaboost/'\n",
            "/content/gdrive/MyDrive/Adaboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scikit-learn pandas matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAc4DO7cG0I",
        "outputId": "11f4d55f-a6f0-4883-f045-d9adbd43be8d"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "odlbMuIWcOph"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import minmax_scale, StandardScaler, MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "GMz5GFS5cMXD"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "dataset.drop(['customerID'] , inplace=True , axis='columns')\n",
        "\n",
        "for i in dataset.columns:\n",
        "  #print(i, dataset[i].nunique())\n",
        "  if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):\n",
        "    #print(dataset[i].unique()[0], dataset[i].unique()[1])\n",
        "    dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})\n",
        "\n",
        "dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')\n",
        "\n",
        "# mean = dataset['TotalCharges'].mean()\n",
        "# dataset['TotalCharges'] = dataset['TotalCharges'].replace([np.nan],mean)\n",
        "arr = []\n",
        "for i in range(len(dataset['tenure'])):\n",
        "  if(dataset.loc[i , 'tenure'] == 0):\n",
        "    imagined_tenure = random.random()\n",
        "    # print(imagined_tenure)\n",
        "    # print(dataset.loc[i , ['MonthlyCharges']])\n",
        "    the_loc = dataset.loc[i , ['MonthlyCharges']]\n",
        "    # print(the_loc)\n",
        "    dataset.at[i , ['TotalCharges']] = float(the_loc * imagined_tenure)\n",
        "    # print(dataset.loc[i , ['TotalCharges']])\n",
        "    arr.append(i)\n",
        "#dataset.info()\n",
        "# for i in dataset['TotalCharges']:\n",
        "#   if(np.isnan(i)):\n",
        "#     i.replace(mean)\n",
        "\n",
        "dataset['Churn'] = dataset['Churn'].map({1: 1, 0: -1})\n",
        "print(dataset['Churn'].unique())\n",
        "\n",
        "\n",
        "dataset = pd.get_dummies(dataset)\n",
        "#print(dataset.head())\n",
        "\n",
        "\n",
        "def normalize(dataset):\n",
        "  for i in dataset.columns:\n",
        "    if(dataset[i].nunique()!=2):\n",
        "      dataset[i] = (dataset[i]-dataset[i].min())/(dataset[i].max()-dataset[i].min())\n",
        "  return dataset\n",
        "\n",
        "#dataset = normalize(dataset)\n",
        "\n",
        "\n",
        "\n",
        "dataset = dataset[3025:5050]\n",
        "# dataset.head(100)\n",
        "\n",
        "#dataset['TotalCharges'].count()\n",
        "\n",
        "#pd.Index(dataset['Churn']).value_counts()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzSdN8Hr9-Ta",
        "outputId": "e4139e73-b6f8-44f6-9a08-eedba8e79d59"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1  1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.at[0, ['tenure']] = 0.2\n",
        "# dataset.loc[0 , ['tenure']]\n",
        "# arr = []\n",
        "# for i in range(len(dataset['tenure'])):\n",
        "#   if(dataset.loc[i , 'tenure'] == 0):\n",
        "#     imagined_tenure = random.random()\n",
        "#     dataset.at[i , ['TotalCharges']] = dataset.loc[i , ['MonthlyCharges']] * imagined_tenure\n",
        "#     arr.append(i)\n",
        "# dataset.loc[488 ,['TotalCharges']]\n"
      ],
      "metadata": {
        "id": "tbJaa_fI0ZVE"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = dataset.drop(['Churn'], axis='columns')\n",
        "\n",
        "y = dataset['Churn']\n",
        "\n",
        "# x\n",
        "# y\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "x_train = normalize(x_train)\n",
        "x_test = normalize(x_test)"
      ],
      "metadata": {
        "id": "ABZkromHYXJX"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1aEa7LRfbMa0"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Tanh(z):\n",
        "    th = np.tanh(z)\n",
        "    # if(np.isnan(th)):\n",
        "    #   if(z < 0):\n",
        "    #     return -1\n",
        "    #   else:\n",
        "    #     return 1\n",
        "\n",
        "    return float(th)"
      ],
      "metadata": {
        "id": "yWgRALXCxR9a"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesistanh(theta,x):\n",
        "    #print ('theta ', theta)\t\n",
        "    # z = 0\n",
        "    # for i in range(len(theta)):\n",
        "    #     xi = x[i]\n",
        "    #     z += np.dot(xi,theta[i].transpose())\n",
        "\n",
        "    z = np.dot(theta,x)\n",
        "    return Tanh(z)"
      ],
      "metadata": {
        "id": "VY4Ije16m2hS"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha):\n",
        "    sumErrors = 0\n",
        "    for i in range(m):\n",
        "        xi = x_train.iloc[i]\n",
        "        hi = hypothesistanh(theta,xi)\n",
        "        yt = y_train.iloc[i]\n",
        "        hit = np.subtract((float)(hi),float(yt))\n",
        "        error = np.dot(hit,x_train.iloc[i][j])\n",
        "        error = np.dot(error,(1-hi*hi))\n",
        "        sumErrors += error\n",
        "    #m = len(y_test)\n",
        "    constant = float(alpha)/float(m)\n",
        "    j = constant * sumErrors\n",
        "    #print(\"SumErrors\", sumErrors)\n",
        "    return j"
      ],
      "metadata": {
        "id": "2GO8uISum_ap"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descenttanh(x_train,y_train,theta,m,alpha):\n",
        "    new_theta = []\n",
        "    for j in range(len(theta)):\n",
        "        new_theta_value = theta[j] - cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha)\n",
        "        new_theta.append(new_theta_value)\n",
        "    #print ('theta ', new_theta)\t\n",
        "    return new_theta"
      ],
      "metadata": {
        "id": "FaRpfB-YnwMB"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error_calc(x_train,y_train,theta):\n",
        "  total = 0\n",
        "  m = len(y_train)\n",
        "  for i in range(m):\n",
        "        xi = x_train.iloc[i]\n",
        "        hi = hypothesistanh(theta,xi)\n",
        "        yt = y_train.iloc[i]\n",
        "        hit = np.subtract((float)(hi),float(yt))\n",
        "        total += hit*hit\n",
        "\n",
        "  return total/float(m)"
      ],
      "metadata": {
        "id": "_KQuI23hmkKa"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADVG8TrLuvSa",
        "outputId": "ca6c2348-566d-4365-aad5-1da8f2f10963"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "405"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_tanh(x_train,y_train,alpha,theta,num_iters):\n",
        "    m = len(y_train)\n",
        "    for x in range(num_iters):\n",
        "        new_theta = gradient_descenttanh(x_train,y_train,theta,m,alpha)\n",
        "        theta = new_theta\n",
        "        #total = error_calc(theta)\n",
        "        #if x % 1000 == 0:\n",
        "            #cost_functiontanh(theta,m)\n",
        "        #print ('theta ', theta)\n",
        "        #print('total' , total)\t\n",
        "            #print ('cost is ', cost_functiontanh(theta,m))\n",
        "    score = 0\n",
        "    length = len(x_test)\n",
        "    for i in range(length):\n",
        "        prediction = hypothesistanh(theta,x_test.iloc[i])\n",
        "        if prediction < 0:\n",
        "          prediction = -1\n",
        "        else:\n",
        "          prediction = 1\n",
        "        # if(prediction == -1):\n",
        "        #   print(\"prediction \", prediction)\n",
        "        answer = y_test.iloc[i]\n",
        "        if prediction == answer:\n",
        "            score += 1\n",
        "    my_score = float(score) / float(length)\n",
        "    print ('Your score with tanh: ', my_score)\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "0Jbgusx2n8Y4"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ada_boost(num_iter):\n",
        "  m = len(y_train)\n",
        "\n",
        "  weight = np.empty(m)\n",
        "  weight.fill(1.0/float(m))\n",
        "  #print(weight)\n",
        "  \n",
        "  hypothesis_container = []\n",
        "  hypothesis_weight = []\n",
        "\n",
        "  np.random.seed(5)\n",
        "\n",
        "  x_train_sampled = x_train\n",
        "  y_train_sampled = y_train\n",
        "\n",
        "  for i in range(num_iter):\n",
        "    \n",
        "    resampled_int = np.random.choice(x_train_sampled.shape[0], size=m, p=weight)\n",
        "    #print(resampled_int)\n",
        "    x_train_sampled = x_train_sampled.iloc[resampled_int]\n",
        "    y_train_sampled = y_train_sampled.iloc[resampled_int]\n",
        "    #print(temp)\n",
        "    #print(x_train_sampled)\n",
        "    \n",
        "    theta = logistic_regression_tanh(x_train_sampled, y_train_sampled, alpha, initial_theta ,iterations)\n",
        "    #hypothesis_container.append(theta)\n",
        "\n",
        "    error = 0\n",
        "\n",
        "    for j in range(m):\n",
        "      xj = x_train_sampled.iloc[j]\n",
        "      hj = hypothesistanh(theta,xj)\n",
        "      if hj < 0:\n",
        "        hj = -1\n",
        "      else:\n",
        "        hj = 1\n",
        "      yj = y_train_sampled.iloc[j]\n",
        "      if hj != yj:\n",
        "        error = error + weight[j]\n",
        "\n",
        "    if error > 0.5:\n",
        "      print(error)\n",
        "      continue\n",
        "\n",
        "    for j in range(m):\n",
        "      xj = x_train_sampled.iloc[j]\n",
        "      hj = hypothesistanh(theta,xj)\n",
        "      if hj < 0:\n",
        "        hj = -1\n",
        "      else:\n",
        "        hj = 1\n",
        "      yj = y_train_sampled.iloc[j]\n",
        "      if hj == yj:\n",
        "        weight[j] = weight[j] * float(error)/(1.0-float(error))\n",
        "\n",
        "    weight = weight/np.sum(weight)\n",
        "    #print(weight)\n",
        "\n",
        "    hypothesis_container.append(theta)\n",
        "    hypothesis_weight.append(np.log2(  (1.0-float(error))   /   float(error)   ))\n",
        "\n",
        "  h_cap = Weighted_majority(x_test , hypothesis_container , hypothesis_weight)\n",
        "  #print(h_cap)\n",
        "\n",
        "  m2 = len(x_test)\n",
        "  score = 0\n",
        "  for i in range(m2):\n",
        "    if h_cap[i] < 0:\n",
        "      h_cap[i] = -1\n",
        "    else:\n",
        "      h_cap[i] = 1\n",
        "\n",
        "    answer = y_test.iloc[i]\n",
        "    if h_cap[i] == answer:\n",
        "      score += 1\n",
        "  print(h_cap)\n",
        "  print(float(score) / float(m2))\n",
        "\n",
        "\n",
        "def Weighted_majority(x , hypothesis_container , hypothesis_weight):\n",
        "  m = len(x)\n",
        "  iter = len(hypothesis_weight)\n",
        "  #print(iter)\n",
        "  h_cap = np.zeros(m)\n",
        "  for k in range(iter):\n",
        "    for i in range(m):\n",
        "      xi = x.iloc[i]\n",
        "      hi = hypothesistanh(hypothesis_container[k] , xi)\n",
        "      mul = hi*hypothesis_weight[k]\n",
        "      h_cap[i] = h_cap[i] + mul\n",
        "\n",
        "  #print(h_cap)\n",
        "  return h_cap\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KP0a_9a6qma9"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_theta = np.zeros(x_test.iloc[1].shape)\n",
        "#x_test.iloc[1]\n",
        "alpha = 0.5\n",
        "iterations = 10\n",
        "#logistic_regression_tanh(alpha,initial_theta,iterations)\n",
        "ada_boost(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F14TE4Kjohh4",
        "outputId": "46005cec-8b0d-42fd-af3f-e1cb852d7014"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your score with tanh:  0.7802469135802469\n",
            "Your score with tanh:  0.6049382716049383\n",
            "Your score with tanh:  0.691358024691358\n",
            "Your score with tanh:  0.2617283950617284\n",
            "Your score with tanh:  0.28641975308641976\n",
            "[ 0.28458526 -0.26670753 -2.3203746  -2.20607321 -2.43018158  0.25584425\n",
            " -1.52260287 -0.56768495  0.69459707 -0.90514636 -0.17806645 -2.07873039\n",
            " -2.91504908  0.77113599  0.35043315 -1.49596852 -2.54870014  0.32701195\n",
            " -2.1402917  -0.02945547  1.17432068  0.58533855 -2.77422334 -1.84608269\n",
            " -0.43270851 -0.20936275 -2.13740409 -0.66264884 -2.953478   -0.28452405\n",
            " -2.37734452 -1.97391035 -2.2419813   0.48498265 -1.52869945  0.04330297\n",
            "  1.26003185 -2.20407525  0.86886152  0.99197676 -0.8424172  -2.3031283\n",
            " -0.68916057 -1.84830069 -2.72009491  0.77847229  0.63828499 -0.83519447\n",
            "  0.15780274 -2.01271388 -3.0208987  -1.29505271 -1.44156708  0.27142106\n",
            " -0.33787034 -0.10543592  0.59023421 -2.23894641 -0.83150747  0.63444288\n",
            " -0.43385583 -1.11335299 -1.85353801 -0.4186874  -0.05836717 -2.57921786\n",
            " -2.35661908 -0.1728455  -1.4226983  -2.3745264   1.29716421 -2.23361606\n",
            " -0.94593367  0.28787551  0.44525006  0.7980955  -1.71164501 -1.34940201\n",
            " -2.30397518 -0.39148353 -2.4750089  -2.99997339 -2.03980248 -2.35318504\n",
            " -1.52388766  0.77844605 -1.27457072 -1.58421691  1.38319786  0.69875064\n",
            "  0.09201421 -2.6075081  -2.98791153 -0.32893274  0.36733612  0.77913516\n",
            " -0.6624602   0.74226087 -2.42182938 -0.95098194 -1.70388083 -1.08229693\n",
            " -0.49277065 -2.41193764 -0.19368542 -2.55293497 -0.76442869 -2.52910449\n",
            "  0.74992324 -0.23694239  0.78712992 -0.26995361 -2.59972193 -1.43840227\n",
            " -1.75036643 -1.39371534  1.49656622 -1.88023191 -1.16267389 -0.09439916\n",
            " -0.21258141 -0.1889957  -2.09145624  0.07726014 -2.44232204 -0.7163976\n",
            "  0.85046269 -1.58076384  0.34107061  0.18318319 -0.21859642 -1.44311228\n",
            " -0.71499019  1.25017983 -0.08370732  0.88509162  0.67010185  0.63815893\n",
            " -0.57180419  0.66901146 -0.62535138 -0.05330208  0.23108794 -1.77482275\n",
            " -2.03798489  0.64555033  0.25256714 -2.95408506  0.41148547  0.72540279\n",
            " -1.81388932 -2.87903522  1.02966776 -2.85929124 -2.27354693 -1.54928477\n",
            "  0.92911789 -0.58216455 -2.06772318 -2.66579734 -1.43845117 -1.534678\n",
            " -0.1868575  -0.15288041 -0.35946135 -1.43837171 -2.55794547 -2.98790478\n",
            " -0.04327825  0.54396732 -0.74351022 -0.25933331  0.9789953  -1.91132525\n",
            "  0.21412859 -1.49830745 -1.43829222  0.19479035 -1.52922644 -1.8217636\n",
            " -2.36791852 -0.56232305 -0.84793825  0.71309879 -1.59829264  0.70924267\n",
            " -2.7370048   0.52653876  0.80411819 -1.87930645 -2.11228738 -0.22152071\n",
            " -1.8810257   0.02207693 -0.40936004 -1.84756724  0.9040792  -0.46590409\n",
            " -2.27445018 -2.36296557 -2.95100731 -2.4274739  -3.06352306 -2.61264849\n",
            " -0.27772661  0.32846782 -2.53142471 -2.99218555 -2.08892101 -2.26144675\n",
            " -1.87595414  0.59278633  1.34338058 -0.90033294 -0.56699379 -1.20508715\n",
            " -2.90011853 -2.4350893   0.47276154 -1.25506987 -1.24568035  0.73444129\n",
            "  1.00624534 -1.63325854 -2.48311198 -1.79427323 -0.08559168 -1.6968344\n",
            " -1.7143903  -1.79509019 -0.07027445  0.84461862 -0.93804607 -1.58596324\n",
            " -1.92769297 -2.79307147  0.67908232 -2.97594412  0.12054153 -1.56324459\n",
            "  0.30611268 -0.47129647 -1.43302507  0.90030181 -1.92398483  0.88084045\n",
            "  0.08341389 -1.87171059 -2.00160379 -2.69162207 -1.57919412 -1.12607835\n",
            " -0.7502123  -2.77769807 -2.07828495 -0.42553828  0.04205639 -2.34909781\n",
            " -2.73040779 -1.99418217 -1.21120401  0.15942741 -1.59513337  0.49602798\n",
            "  0.20216245 -1.01128354 -2.91806675  1.06916259 -1.46003681 -0.14993072\n",
            " -0.26200887 -0.14000621  0.24160559 -1.79928901  0.63689246 -2.0943521\n",
            " -1.17394622  0.56065966 -1.53535464 -2.276154    0.15780611 -2.65519195\n",
            "  1.03817222  0.6300647   0.88918816 -1.99275936 -1.28490754 -1.27766632\n",
            " -0.22050697 -1.7578387  -1.04977109  0.45114081 -1.21978927 -1.69145886\n",
            " -2.83908596  0.86098579 -1.37654601  0.63306966 -1.95179114 -1.99991221\n",
            " -2.61616355 -1.23867386 -1.93245983  0.78428539  0.36328377 -0.95538915\n",
            " -1.54053601 -1.09565589  0.19405187 -0.80858454 -1.61733874  0.41025649\n",
            "  0.48925785 -1.48696285 -1.47861323 -2.65550755 -0.5246429   0.77122978\n",
            " -2.47229531 -1.19322467  0.24660037 -1.07919489  0.58337803 -2.09609421\n",
            " -0.96621672 -0.66272588  0.67072494  0.23579655 -1.57031735 -2.69703194\n",
            "  0.84978236  0.76600268  0.34646644 -0.43347837 -1.38278652 -1.41764177\n",
            " -1.19001247 -1.06789999  0.01311262  1.14840009 -3.06259113  0.64796865\n",
            " -0.31024973 -0.44983707 -2.16883866  1.17653072 -1.75760264 -2.42629844\n",
            " -2.19883991  0.74562776 -1.16268002 -1.57685517 -0.34205899  0.11539723\n",
            " -1.01999952  0.15415255 -0.93475113 -1.49841241  0.93187576 -0.4233253\n",
            " -0.34521284  1.33533593 -0.21150578 -2.81596218 -0.41246128  0.7869736\n",
            "  0.29396487 -1.22049019  0.45480566 -1.58470688 -0.81059944 -2.06409002\n",
            " -0.726077   -2.34419284 -2.3769541   0.75859829  0.65164614 -0.05826422\n",
            "  0.11845618  0.86921844 -0.70181561  0.91267661  0.99082365  0.69618617\n",
            " -2.23465767 -0.67934969  0.7175749  -1.9092778  -0.0333622  -1.59119785\n",
            " -2.55898543 -2.20526724 -2.0805952   0.33153152 -0.39933207  0.81506441\n",
            " -1.15094637 -0.26844089 -0.17978349 -3.04787266 -1.34681279  1.02474753\n",
            " -1.61196041  0.54061557 -0.10057984]\n",
            "[ 1. -1. -1. -1. -1.  1. -1. -1.  1. -1. -1. -1. -1.  1.  1. -1. -1.  1.\n",
            " -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1.\n",
            "  1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1.\n",
            " -1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
            " -1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
            "  1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
            "  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
            "  1. -1.  1.  1. -1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1.  1. -1.\n",
            " -1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1.\n",
            " -1. -1. -1.  1. -1.  1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1.\n",
            " -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.\n",
            " -1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1. -1.\n",
            " -1. -1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1. -1.\n",
            " -1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1.  1.  1. -1.\n",
            " -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
            " -1. -1.  1.  1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1.  1.  1. -1.  1.\n",
            " -1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1.\n",
            " -1.  1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1. -1.\n",
            "  1.  1. -1.  1.  1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1. -1.  1.\n",
            " -1. -1. -1. -1. -1.  1. -1.  1. -1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bal = np.array([])\n",
        "# counter = 0\n",
        "# abal = [1,2,3,4,5]\n",
        "# dhon = [3,4,6,7,8,9]\n",
        "# bal.append(abal)\n",
        "# bal.append(dhon)\n",
        "# len(bal[1])\n",
        "# bal = np.empty(5)\n",
        "# bal.fill(1/5)\n",
        "# bal\n",
        "# len(y_train)\n",
        "# ada_boost(3)\n",
        "a = np.array([[1, 0],\n",
        "              [0, 1]])\n",
        "\n",
        "a[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVp2gg4WOnr_",
        "outputId": "41aa0ae5-9fa9-4e65-a8c6-f6c7d8ddc876"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,2,3,4,5]\n",
        "b = [1,2,3,4,5]\n",
        "c = np.dot(a,b)\n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70iQXgulxX1w",
        "outputId": "273fe441-a88b-475c-cd6a-0f08a3b46654"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    }
  ]
}