# -*- coding: utf-8 -*-
"""1605020 _adult_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mb6fvLWGVcJGtJhAxqpqXQsgVvVE2cBx
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/Adaboost/

# !pip install numpy scikit-learn pandas matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
import random

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
from matplotlib import pyplot as plt
# %matplotlib inline

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import minmax_scale, StandardScaler, MinMaxScaler
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')

dataset.drop(['customerID'] , inplace=True , axis='columns')

for i in dataset.columns:
  #print(i, dataset[i].nunique())
  if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):
    #print(dataset[i].unique()[0], dataset[i].unique()[1])
    dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})

dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')

# mean = dataset['TotalCharges'].mean()
# dataset['TotalCharges'] = dataset['TotalCharges'].replace([np.nan],mean)
arr = []
for i in range(len(dataset['tenure'])):
  if(dataset.loc[i , 'tenure'] == 0):
    imagined_tenure = random.random()
    # print(imagined_tenure)
    # print(dataset.loc[i , ['MonthlyCharges']])
    the_loc = dataset.loc[i , ['MonthlyCharges']]
    # print(the_loc)
    dataset.at[i , ['TotalCharges']] = float(the_loc * imagined_tenure)
    # print(dataset.loc[i , ['TotalCharges']])
    arr.append(i)
#dataset.info()
# for i in dataset['TotalCharges']:
#   if(np.isnan(i)):
#     i.replace(mean)

dataset['Churn'] = dataset['Churn'].map({1: 1, 0: -1})
print(dataset['Churn'].unique())


dataset = pd.get_dummies(dataset)
#print(dataset.head())


def normalize(dataset):
  for i in dataset.columns:
    if(dataset[i].nunique()!=2):
      dataset[i] = (dataset[i]-dataset[i].min())/(dataset[i].max()-dataset[i].min())
  return dataset

#dataset = normalize(dataset)



#dataset = dataset[3025:5050]
# dataset.head(100)

#dataset['TotalCharges'].count()

#pd.Index(dataset['Churn']).value_counts()

# dataset.at[0, ['tenure']] = 0.2
# dataset.loc[0 , ['tenure']]
# arr = []
# for i in range(len(dataset['tenure'])):
#   if(dataset.loc[i , 'tenure'] == 0):
#     imagined_tenure = random.random()
#     dataset.at[i , ['TotalCharges']] = dataset.loc[i , ['MonthlyCharges']] * imagined_tenure
#     arr.append(i)
# dataset.loc[488 ,['TotalCharges']]

x = dataset.drop(['Churn'], axis='columns')

y = dataset['Churn']

# x
# y

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

x_train = normalize(x_train)
x_test = normalize(x_test)



def Tanh(z):
    th = np.tanh(z)
    # if(np.isnan(th)):
    #   if(z < 0):
    #     return -1
    #   else:
    #     return 1

    return float(th)

def hypothesistanh(theta,x):
    #print ('theta ', theta)	
    # z = 0
    # for i in range(len(theta)):
    #     xi = x[i]
    #     z += np.dot(xi,theta[i].transpose())

    z = np.dot(theta,x)
    return Tanh(z)

def cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha):
    sumErrors = 0
    for i in range(m):
        xi = x_train.iloc[i]
        hi = hypothesistanh(theta,xi)
        yt = y_train.iloc[i]
        hit = np.subtract((float)(hi),float(yt))
        error = np.dot(hit,x_train.iloc[i][j])
        error = np.dot(error,(1-hi*hi))
        sumErrors += error
    #m = len(y_test)
    constant = float(alpha)/float(m)
    j = constant * sumErrors
    #print("SumErrors", sumErrors)
    return j

def gradient_descenttanh(x_train,y_train,theta,m,alpha):
    new_theta = []
    for j in range(len(theta)):
        new_theta_value = theta[j] - cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha)
        new_theta.append(new_theta_value)
    #print ('theta ', new_theta)	
    return new_theta

def error_calc(x_train,y_train,theta):
  total = 0
  m = len(y_train)
  for i in range(m):
        xi = x_train.iloc[i]
        hi = hypothesistanh(theta,xi)
        yt = y_train.iloc[i]
        hit = np.subtract((float)(hi),float(yt))
        total += hit*hit

  return total/float(m)

len(x_test)

def logistic_regression_tanh(x_train,y_train,alpha,theta,num_iters):
    m = len(y_train)
    for x in range(num_iters):
        new_theta = gradient_descenttanh(x_train,y_train,theta,m,alpha)
        theta = new_theta
        #total = error_calc(theta)
        #if x % 1000 == 0:
            #cost_functiontanh(theta,m)
        #print ('theta ', theta)
        #print('total' , total)	
            #print ('cost is ', cost_functiontanh(theta,m))
    score = 0
    length = len(x_test)
    for i in range(length):
        prediction = hypothesistanh(theta,x_test.iloc[i])
        if prediction < 0:
          prediction = -1
        else:
          prediction = 1
        # if(prediction == -1):
        #   print("prediction ", prediction)
        answer = y_test.iloc[i]
        if prediction == answer:
            score += 1
    my_score = float(score) / float(length)
    print ('Your score with tanh: ', my_score)

    return theta

def ada_boost(num_iter):
  m = len(y_train)

  weight = np.empty(m)
  weight.fill(1.0/float(m))
  #print(weight)
  
  hypothesis_container = []
  hypothesis_weight = []

  np.random.seed(5)

  x_train_sampled = x_train
  y_train_sampled = y_train

  for i in range(num_iter):
    
    resampled_int = np.random.choice(x_train_sampled.shape[0], size=m, p=weight)
    #print(resampled_int)
    x_train_sampled = x_train_sampled.iloc[resampled_int]
    y_train_sampled = y_train_sampled.iloc[resampled_int]
    #print(temp)
    #print(x_train_sampled)
    
    theta = logistic_regression_tanh(x_train_sampled, y_train_sampled, alpha, initial_theta ,iterations)
    #hypothesis_container.append(theta)

    error = 0

    for j in range(m):
      xj = x_train_sampled.iloc[j]
      hj = hypothesistanh(theta,xj)
      if hj < 0:
        hj = -1
      else:
        hj = 1
      yj = y_train_sampled.iloc[j]
      if hj != yj:
        error = error + weight[j]

    if error > 0.5:
      print(error)
      continue

    for j in range(m):
      xj = x_train_sampled.iloc[j]
      hj = hypothesistanh(theta,xj)
      if hj < 0:
        hj = -1
      else:
        hj = 1
      yj = y_train_sampled.iloc[j]
      if hj == yj:
        weight[j] = weight[j] * float(error)/(1.0-float(error))

    weight = weight/np.sum(weight)
    #print(weight)

    hypothesis_container.append(theta)
    hypothesis_weight.append(np.log2(  (1.0-float(error))   /   float(error)   ))

  h_cap = Weighted_majority(x_test , hypothesis_container , hypothesis_weight)
  #print(h_cap)

  m2 = len(x_test)
  score = 0
  for i in range(m2):
    if h_cap[i] < 0:
      h_cap[i] = -1
    else:
      h_cap[i] = 1

    answer = y_test.iloc[i]
    if h_cap[i] == answer:
      score += 1
  print(h_cap)
  print(float(score) / float(m2))


def Weighted_majority(x , hypothesis_container , hypothesis_weight):
  m = len(x)
  iter = len(hypothesis_weight)
  #print(iter)
  h_cap = np.zeros(m)
  for k in range(iter):
    for i in range(m):
      xi = x.iloc[i]
      hi = hypothesistanh(hypothesis_container[k] , xi)
      mul = hi*hypothesis_weight[k]
      h_cap[i] = h_cap[i] + mul

  #print(h_cap)
  return h_cap

initial_theta = np.zeros(x_test.iloc[1].shape)
#x_test.iloc[1]
alpha = 0.5
iterations = 10
#logistic_regression_tanh(alpha,initial_theta,iterations)
ada_boost(5)

# bal = np.array([])
# counter = 0
# abal = [1,2,3,4,5]
# dhon = [3,4,6,7,8,9]
# bal.append(abal)
# bal.append(dhon)
# len(bal[1])
# bal = np.empty(5)
# bal.fill(1/5)
# bal
# len(y_train)
# ada_boost(3)
a = np.array([[1, 0],
              [0, 1]])

a[0]

a = [1,2,3,4,5]
b = [1,2,3,4,5]
c = np.dot(a,b)
c