{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adaboost.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC1U04CHb9fh",
        "outputId": "ceaee624-682f-44af-efeb-c9081ba6593d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/AdaBoost/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xdFnWSQcv8h",
        "outputId": "a154a7fb-a6fe-4fba-e9c5-3e54f95d48fb"
      },
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/MyDrive/AdaBoost/'\n",
            "/content/gdrive/MyDrive/AdaBoost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scikit-learn pandas matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAc4DO7cG0I",
        "outputId": "f56faff8-c294-4fe8-c1d5-b1b18f89877b"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(2021)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "odlbMuIWcOph"
      },
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(dataset):\n",
        "  for i in dataset.columns:\n",
        "    if(dataset[i].nunique() > 2):\n",
        "      dataset[i] = (dataset[i]-dataset[i].min())/(dataset[i].max()-dataset[i].min())\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "TLzL49JyfOH0"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_yes_no(x):\n",
        "  if x =='Yes' or x == 'Male':\n",
        "      return 1\n",
        "  elif x == 'No' or x == 'Female' :\n",
        "      return 0\n",
        "  else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "NOkyr9Y3ggFM"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess1():\n",
        "  dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "  dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'] , errors='coerce')\n",
        "\n",
        "  for i in range(len(dataset['tenure'])):\n",
        "    if(dataset.loc[i , 'tenure'] == 0):\n",
        "      probable_tenure = random.random()\n",
        "      monthlyCharge = dataset.loc[i , ['MonthlyCharges']]\n",
        "      dataset.at[i , ['TotalCharges']] = float(probable_tenure * monthlyCharge)\n",
        "\n",
        "  dataset.drop(['customerID'] , inplace=True , axis='columns')\n",
        "\n",
        "  object_col=[]\n",
        "  num_col=[]\n",
        "  for i in dataset.columns:\n",
        "    if dataset[i].dtype == 'object':\n",
        "      object_col.append(i)\n",
        "    else:\n",
        "      num_col.append(i)\n",
        "\n",
        "  for i in object_col:\n",
        "    if len(dataset[i].unique()) == 2:\n",
        "      dataset[i] = dataset[i].map(change_yes_no)\n",
        "\n",
        "  dataset['Churn'] = dataset['Churn'].map({1: 1, 0: -1})\n",
        "  \n",
        "  dataset_dummies = pd.get_dummies(dataset)\n",
        "\n",
        "  return dataset_dummies\n"
      ],
      "metadata": {
        "id": "NcTLELg6fG4N"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess2():\n",
        "  dataset = pd.read_csv('adult.csv')\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    maximum_occured = dataset[i].value_counts().idxmax()\n",
        "    dataset[i] = dataset[i].replace(' ?', maximum_occured)\n",
        "\n",
        "  dataset[' Income'] = dataset[' Income'].map({' <=50K': -1, ' >50K': 1})\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):\n",
        "      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})\n",
        "\n",
        "  dataset = pd.get_dummies(dataset)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "v4PCT3SotvtM"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess2_2():\n",
        "  dataset = pd.read_csv('adult_test.csv')\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    maximum_occured = dataset[i].value_counts().idxmax()\n",
        "    dataset[i] = dataset[i].replace(' ?', maximum_occured)\n",
        "\n",
        "  dataset[' Income'] = dataset[' Income'].map({' <=50K.': -1, ' >50K.': 1})\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):\n",
        "      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})\n",
        "\n",
        "  dataset = pd.get_dummies(dataset)\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "bub3dJn9qzTQ"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess3():\n",
        "  dataset = pd.read_csv('creditcard.csv')\n",
        "\n",
        "  dataset['Class'] = dataset['Class'].map({1: 1, 0: -1})\n",
        "\n",
        "  dataset_with_all_one = dataset[dataset['Class'] == 1]\n",
        "\n",
        "  class_one_index = dataset_with_all_one.index\n",
        "\n",
        "  dataset.drop(class_one_index)\n",
        "\n",
        "  dataset_with_some_zero = dataset.sample(n=500)\n",
        "\n",
        "  dataset_merged = pd.concat([dataset_with_all_one, dataset_with_some_zero])\n",
        "\n",
        "  dataset_merged = shuffle(dataset_merged)\n",
        "\n",
        "  return dataset_merged"
      ],
      "metadata": {
        "id": "e1n-oDGuuIHW"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')"
      ],
      "metadata": {
        "id": "BWeif0hpcaL3"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.head(12)"
      ],
      "metadata": {
        "id": "QIym4D_ocfy4"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.info()"
      ],
      "metadata": {
        "id": "ls_LonA_nGJc"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GTm4EXwAnZSz"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in dataset.columns:\n",
        "#     print(f'{i} : {dataset[i].unique()} \\n \"{dataset[i].dtype}\"')\n",
        "#     print('-----------------------------------------')"
      ],
      "metadata": {
        "id": "mNHT2T7onz1T"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in dataset.columns:\n",
        "#     print(f'unique values in column \"{i}\" is \\n {dataset[i].value_counts()} ')\n",
        "#     print('----------------------------------------------------------')"
      ],
      "metadata": {
        "id": "KoQwt1RQpBIF"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.shape"
      ],
      "metadata": {
        "id": "YN5-X6BAppkb"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.duplicated().sum()"
      ],
      "metadata": {
        "id": "WBQsMrDOp3jL"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.describe()"
      ],
      "metadata": {
        "id": "jq-v6ay6qdKb"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.TotalCharges.dtype"
      ],
      "metadata": {
        "id": "JSu4_HAbsLKq"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'] , errors='coerce')\n",
        "# dataset.TotalCharges.dtype"
      ],
      "metadata": {
        "id": "T2MeaV9rrJRT"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#arr = []\n",
        "# for i in range(len(dataset['tenure'])):\n",
        "#   if(dataset.loc[i , 'tenure'] == 0):\n",
        "#     probable_tenure = random.random()\n",
        "#     #print(imagined_tenure)\n",
        "#     #print(dataset.loc[i , ['MonthlyCharges']])\n",
        "#     monthlyCharge = dataset.loc[i , ['MonthlyCharges']]\n",
        "#     #print(the_loc)\n",
        "#     dataset.at[i , ['TotalCharges']] = float(probable_tenure * monthlyCharge)\n",
        "#     #print(dataset.loc[i , ['TotalCharges']])\n",
        "#     #arr.append(i)\n",
        "# #dataset.info()"
      ],
      "metadata": {
        "id": "gAIa64pT9XOM"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.loc[100 ,['TotalCharges']]\n",
        "# dataset.TotalCharges.dtype"
      ],
      "metadata": {
        "id": "nn0ddPJ0-iYB"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.describe()"
      ],
      "metadata": {
        "id": "bsKRqRv-sVrq"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.columns"
      ],
      "metadata": {
        "id": "_waRyK9lsfxL"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.drop(['customerID'] , inplace=True , axis='columns')"
      ],
      "metadata": {
        "id": "6i6Mu78ItDkC"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset.columns"
      ],
      "metadata": {
        "id": "FWASfeqatlj7"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.style.use('fivethirtyeight')"
      ],
      "metadata": {
        "id": "uMYlaulGtrEr"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# object_col=[]\n",
        "# num_col=[]\n",
        "# for i in dataset.columns:\n",
        "#     if dataset[i].dtype == 'object':\n",
        "#         object_col.append(i)\n",
        "#     else:\n",
        "#         num_col.append(i)"
      ],
      "metadata": {
        "id": "0I7JNHyJuFSc"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#object_col"
      ],
      "metadata": {
        "id": "i1xDqA-dueur"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#num_col"
      ],
      "metadata": {
        "id": "VBXol-b1uk5a"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ax = sns.countplot(x=dataset.Churn)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "3WyVI7nmumPK"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bx = sns.countplot(x=dataset.Partner)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "quFDBaUZafkW"
      },
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in num_col:\n",
        "#     plt.figure(figsize=(7,10))\n",
        "#     sns.boxplot(x=dataset.Churn, y=dataset[i], data=dataset , linewidth=1)\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "CJaS_sNBa6DX"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset[['SeniorCitizen' , 'Churn']]"
      ],
      "metadata": {
        "id": "lZEyaVWVe98m"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cx = sns.countplot(x=dataset.SeniorCitizen , hue=dataset.Churn , data=dataset)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "CYifJqYJfmml"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in object_col:\n",
        "#     plt.figure(figsize=(7,10))\n",
        "#     sns.countplot(x=dataset[i], hue=dataset.Churn, data=dataset , linewidth=1.0)\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "s2iismoCgg8u"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def change_yes_no(x):\n",
        "#     if x =='Yes' or x == 'Male':\n",
        "#         return 1\n",
        "#     elif x == 'No' or x == 'Female' :\n",
        "#         return 0\n",
        "#     else:\n",
        "#         return x\n",
        "\n",
        "# for i in object_col:\n",
        "#     if len(dataset[i].unique()) == 2:\n",
        "#         dataset[i] = dataset[i].map(change_yes_no)"
      ],
      "metadata": {
        "id": "3MK2c83cg6Bt"
      },
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset_dummies = pd.get_dummies(dataset)"
      ],
      "metadata": {
        "id": "i7PSahj1lAic"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in dataset_dummies.columns:\n",
        "#     if(dataset_dummies[i].nunique() != 2):\n",
        "#       dataset_dummies[i] = (dataset_dummies[i]-dataset_dummies[i].min())/(dataset_dummies[i].max()-dataset_dummies[i].min())"
      ],
      "metadata": {
        "id": "GC9jTz9FBBtW"
      },
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix = dataset_dummies.to_numpy()\n",
        "# matrix"
      ],
      "metadata": {
        "id": "NESQc-lumfYE"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dummies = preprocess1()\n",
        "X_base = dataset_dummies.drop('Churn' , axis='columns')\n",
        "y_base = dataset_dummies['Churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_base , y_base , test_size = 0.2 , random_state=0)\n",
        "X_train = normalize(X_train)\n",
        "X_test = normalize(X_test)\n",
        "\n",
        "X_train_array = X_train.values\n",
        "X_test_array = X_test.values\n",
        "y_train_array = y_train.values.reshape(-1,1)\n",
        "y_test_array = y_test.values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "YZbZELfmoR-N"
      },
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_train = preprocess2()\n",
        "# dataset_test = preprocess2_2()\n",
        "\n",
        "# missing_columns = set( dataset_train.columns ) - set( dataset_test.columns )\n",
        "\n",
        "# for c in missing_columns:\n",
        "#   dataset_test[c] = 0\n",
        "\n",
        "# dataset_test = dataset_test[dataset_train.columns]\n",
        "\n",
        "# X_train = dataset_train.drop([' Income'] , axis='columns')\n",
        "# y_train = dataset_train[' Income']\n",
        "\n",
        "# X_test = dataset_test.drop([' Income'] , axis='columns')\n",
        "# y_test = dataset_test[' Income']\n",
        "\n",
        "# X_train = normalize(X_train)\n",
        "# X_test = normalize(X_test)\n",
        "\n",
        "# X_train_array = X_train.values\n",
        "# X_test_array = X_test.values\n",
        "# y_train_array = y_train.values.reshape(-1,1)\n",
        "# y_test_array = y_test.values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "zVLqXgYcovMb"
      },
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = preprocess3()\n",
        "\n",
        "# X_base = dataset.drop('Class' , axis='columns')\n",
        "# y_base = dataset['Class']\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_base , y_base , test_size = 0.2 , random_state=0)\n",
        "# X_train = normalize(X_train)\n",
        "# X_test = normalize(X_test)\n",
        "\n",
        "# X_train_array = X_train.values\n",
        "# X_test_array = X_test.values\n",
        "# y_train_array = y_train.values.reshape(-1,1)\n",
        "# y_test_array = y_test.values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "SsUlz3fCwUvP"
      },
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHal1T5cl88M",
        "outputId": "465d2d41-7ce4-40e1-ac9b-5b5681506f94"
      },
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1409, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 359
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def Tanh(z):\n",
        "#     return float(math.tanh(z))"
      ],
      "metadata": {
        "id": "l2uAF7di-F6r"
      },
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def hypothesis(w,x):\n",
        "#     z = 0\n",
        "#     for i in range(len(w)):\n",
        "#         xi = x[i]\n",
        "#         z += np.dot(xi,w[i])\n",
        "#     return Tanh(z)"
      ],
      "metadata": {
        "id": "47lboPAiTAvy"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def cost_function_derivative(X_train,y_train,w,j,m,alpha):\n",
        "#     sum = 0\n",
        "#     for i in range(m):\n",
        "#         xi = X_train.iloc[i]\n",
        "#         hi = hypothesis(w,xi)\n",
        "#         yt = y_train.iloc[i]\n",
        "#         hit = np.subtract((float)(hi),float(yt))\n",
        "#         error = np.dot(hit,X_train.iloc[i][j])\n",
        "#         error = np.dot(error,(1-hi*hi))\n",
        "#         sum += error\n",
        "#     constant = float(alpha)/float(m)\n",
        "#     j = constant * sum\n",
        "#     return j"
      ],
      "metadata": {
        "id": "Jm2PUHmTCgPs"
      },
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def gradient_descent(X_train,y_train,w,m,alpha):\n",
        "#     new_w = []\n",
        "#     for j in range(len(w)):\n",
        "#         new_w_value = w[j] - cost_function_derivative(X_train,y_train,w,j,m,alpha)\n",
        "#         new_w.append(new_w_value)\t\n",
        "#     return new_w"
      ],
      "metadata": {
        "id": "AuPPYahRDayS"
      },
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def logistic_regression(X_train,y_train,alpha,w,number_of_epoch):\n",
        "#     m = len(y_train)\n",
        "#     for x in range(number_of_epoch):\n",
        "#         new_w = gradient_descent(X_train,y_train,w,m,alpha)\n",
        "#         w = new_w\n",
        "#         print ('weight ', w)\t\n",
        "            \n",
        "#     score = 0\n",
        "#     length = len(X_test)\n",
        "#     for i in range(length):\n",
        "#         prediction = round(hypothesis(w,X_test.iloc[i]))\n",
        "#         answer = y_test.iloc[i]\n",
        "#         if prediction == answer:\n",
        "#             score += 1\n",
        "#     final_score = float(score) / float(length)\n",
        "#     print ('Score with tanh: ', final_score)\n",
        "\n",
        "#     return w"
      ],
      "metadata": {
        "id": "Yuxj_qFjEWRa"
      },
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Tanh(z):\n",
        "    #print(z)\n",
        "    tan_h_done = np.tanh(z)\n",
        "    #print(tan_h_done)\n",
        "    return tan_h_done"
      ],
      "metadata": {
        "id": "7pBsXa7vH83m"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesis(w , X):\n",
        "    z = X @ w\n",
        "    # print('z.shape',z.shape)\n",
        "    return Tanh(z)"
      ],
      "metadata": {
        "id": "dhAJ-8WPIMpl"
      },
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_L2_loss(y , y_hat):\n",
        "  difference = y - y_hat\n",
        "  return np.mean(np.power(difference , 2))"
      ],
      "metadata": {
        "id": "FnN7Y3RRIdB_"
      },
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X , y , y_hat , alpha):\n",
        "    m = X.shape[0]\n",
        "    #dw = (1/m) * ( X.T @ ((y - y_hat) * (1 - y_hat**2)) )\n",
        "    difference = y - y_hat\n",
        "    derivative = 1 - np.power(y_hat , 2)\n",
        "    multiplied = difference * derivative\n",
        "    # print('multi: ', multi.shape)\n",
        "    # print('derivative: ', derivative.shape)\n",
        "    # print('diff: ', diff.shape)\n",
        "    #dw = np.dot(X.T , multi)\n",
        "    dw = X.T @ multiplied\n",
        "    # print('dw: ', dw.shape)\n",
        "    constant = (1/float(m)) * float(alpha) \n",
        "    return constant * dw"
      ],
      "metadata": {
        "id": "CKHUdkYDIqY2"
      },
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X , y , number_of_iterations , alpha , boundary):\n",
        "  m,n = X.shape\n",
        "  w = np.zeros((n+1 , 1))\n",
        "  # print(w.shape)\n",
        "  X_train_now = np.concatenate((np.ones((m, 1)), X) , axis = 1)\n",
        "\n",
        "  for i in range(number_of_iterations):\n",
        "    y_hat = hypothesis(w , X_train_now)\n",
        "    # print('y_hat' , y_hat.shape)\n",
        "    decay = gradient_descent(X_train_now , y , y_hat , alpha)\n",
        "    # print('decay :' , decay.shape)\n",
        "    # print('w_before :' , w.shape)\n",
        "    w = w + decay\n",
        "    # print('w_after :' , w.shape)\n",
        "    error = mean_L2_loss(y , y_hat)\n",
        "\n",
        "    if error < boundary:\n",
        "      #print('bingo', i)\n",
        "      break\n",
        "\n",
        "  match = 0\n",
        "  length = X_test_array.shape[0]\n",
        "  X_test_now = np.concatenate((np.ones((length, 1)), X_test_array), axis=1)\n",
        "  \n",
        "  predicted_labels = hypothesis(w, X_test_now)\n",
        "  # for i in range(prediction.shape[0]):\n",
        "  #   if(np.isnan(prediction[i].any())):\n",
        "  #       print('decay')\n",
        "  #print(predicted_labels)\n",
        "  #print('pred uniq ', np.unique(predicted_labels, axis=0))\n",
        "  \n",
        "  for i in range(length):\n",
        "    if predicted_labels[i][0] < 0:\n",
        "      predicted_labels[i][0] = -1\n",
        "    else:\n",
        "      predicted_labels[i][0] = 1\n",
        "    \n",
        "    real_label = y_test_array[i][0]\n",
        "    if predicted_labels[i][0] == real_label:\n",
        "      match += 1\n",
        "\n",
        "  #print('pred uniq ', np.unique(prediction, axis=0))\n",
        "  accuracy = float(match) / float(length)\n",
        "  print ('Accuracy: ', accuracy)\n",
        "  \n",
        "  return w"
      ],
      "metadata": {
        "id": "vZZA7087Jumm"
      },
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression():\n",
        "  w = logistic_regression(X_train_array , y_train_array , iterations , alpha , 0.5)\n",
        "  \n",
        "  length = X_test_array.shape[0]\n",
        "  X_test_now = np.concatenate((np.ones((length, 1)), X_test_array), axis=1)\n",
        "  predicted_labels = hypothesis(w, X_test_now)\n",
        "  for i in range(length):\n",
        "    if predicted_labels[i][0] < 0:\n",
        "      predicted_labels[i][0] = -1\n",
        "    else:\n",
        "      predicted_labels[i][0] = 1\n",
        "  CM = confusion_matrix(y_test_array , predicted_labels)\n",
        "  print(CM)\n",
        "  TN = CM[0][0]\n",
        "  FP = CM[0][1]\n",
        "  FN = CM[1][0]\n",
        "  TP = CM[1][1]\n",
        "  #print('accuracy_score : ' , accuracy_score(y_incoming , y_cap))\n",
        "  #print(float(match) / float(m2))\n",
        "  print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))\n",
        "  sensitivity = TP/(TP+FN)\n",
        "  print('sensitivity :' , sensitivity)\n",
        "  specificity = TN/(TN+FP)\n",
        "  print('specificity :' , specificity)\n",
        "  precision = TP/(TP+FP)\n",
        "  print('precision :' , precision)\n",
        "  false_discovery_rate = 1 - precision\n",
        "  print('false_discovery_rate :' , false_discovery_rate)\n",
        "  f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))\n",
        "  print('f1_score :' , f1_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  length = X_train_array.shape[0]\n",
        "  X_test_now = np.concatenate((np.ones((length, 1)), X_train_array), axis=1)\n",
        "  predicted_labels2 = hypothesis(w, X_test_now)\n",
        "  for i in range(length):\n",
        "    if predicted_labels2[i][0] < 0:\n",
        "      predicted_labels2[i][0] = -1\n",
        "    else:\n",
        "      predicted_labels2[i][0] = 1\n",
        "  CM = confusion_matrix(y_train_array , predicted_labels2)\n",
        "  print(CM)\n",
        "  TN = CM[0][0]\n",
        "  FP = CM[0][1]\n",
        "  FN = CM[1][0]\n",
        "  TP = CM[1][1]\n",
        "  #print('accuracy_score : ' , accuracy_score(y_incoming , y_cap))\n",
        "  #print(float(match) / float(m2))\n",
        "  print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))\n",
        "  sensitivity = TP/(TP+FN)\n",
        "  print('sensitivity :' , sensitivity)\n",
        "  specificity = TN/(TN+FP)\n",
        "  print('specificity :' , specificity)\n",
        "  precision = TP/(TP+FP)\n",
        "  print('precision :' , precision)\n",
        "  false_discovery_rate = 1 - precision\n",
        "  print('false_discovery_rate :' , false_discovery_rate)\n",
        "  f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))\n",
        "  print('f1_score :' , f1_score)\n",
        "\n",
        "test_logistic_regression()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyagzDr_zdeG",
        "outputId": "82e16a81-638c-4b2b-ffb1-a067820d936b"
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.7977288857345636\n",
            "[[941 100]\n",
            " [185 183]]\n",
            "accuracy : 0.7977288857345636\n",
            "sensitivity : 0.49728260869565216\n",
            "specificity : 0.9039385206532181\n",
            "precision : 0.6466431095406361\n",
            "false_discovery_rate : 0.3533568904593639\n",
            "f1_score : 0.5622119815668202\n",
            "[[3722  411]\n",
            " [ 700  801]]\n",
            "accuracy : 0.8028044018459354\n",
            "sensitivity : 0.5336442371752165\n",
            "specificity : 0.9005564964916526\n",
            "precision : 0.6608910891089109\n",
            "false_discovery_rate : 0.3391089108910891\n",
            "f1_score : 0.5904902322152599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_majority(X , hc , z):\n",
        "  m = X.shape[0]\n",
        "  loops = len(z)\n",
        "  \n",
        "  y_cap = np.zeros(m)\n",
        "  for k in range(loops):\n",
        "    h = hypothesis(hc[k].reshape(-1, 1) , X)\n",
        "    for i in range(m):\n",
        "      multiply_by_z = h[i][0]*z[k]\n",
        "      y_cap[i] = y_cap[i] + multiply_by_z\n",
        "\n",
        "  return y_cap"
      ],
      "metadata": {
        "id": "s1j_aFvdRKrP"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrices(X_incoming , y_incoming , hc , z):\n",
        "    X_test_final = np.concatenate((np.ones((X_incoming.shape[0], 1)), X_incoming), axis=1)\n",
        "    y_cap = weighted_majority(X_test_final , hc , z)\n",
        "    #print('accuracy_score : ' , accuracy_score(y_test_array , y_cap))\n",
        "\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    m2 = X_incoming.shape[0]\n",
        "    match = 0\n",
        "    for i in range(m2):\n",
        "      if y_cap[i] < 0:\n",
        "        y_cap[i] = -1\n",
        "      else:\n",
        "        y_cap[i] = 1\n",
        "\n",
        "      original_label = y_incoming[i][0]\n",
        "      if y_cap[i] == original_label:\n",
        "        # if original_label == 1:\n",
        "        #   tp = tp + 1\n",
        "        # else:\n",
        "        #   tn = tn + 1\n",
        "        match += 1\n",
        "    \n",
        "    CM = confusion_matrix(y_incoming , y_cap)\n",
        "    print(CM)\n",
        "    #print(tp , tn)\n",
        "    TN = CM[0][0]\n",
        "    FP = CM[0][1]\n",
        "    FN = CM[1][0]\n",
        "    TP = CM[1][1]\n",
        "    print('accuracy_score : ' , accuracy_score(y_incoming , y_cap))\n",
        "    print(float(match) / float(m2))\n",
        "    print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))\n",
        "    # sensitivity = TP/(TP+FN)\n",
        "    # print('sensitivity :' , sensitivity)\n",
        "    # specificity = TN/(TN+FP)\n",
        "    # print('specificity :' , specificity)\n",
        "    # precision = TP/(TP+FP)\n",
        "    # print('precision :' , precision)\n",
        "    # false_discovery_rate = 1 - precision\n",
        "    # print('false_discovery_rate :' , false_discovery_rate)\n",
        "    # f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))\n",
        "    # print('f1_score :' , f1_score)"
      ],
      "metadata": {
        "id": "fNgYn558JN6Z"
      },
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaBoost(K):\n",
        "  m = y_train_array.shape[0]\n",
        "\n",
        "  weight = np.empty(m)\n",
        "  weight.fill(1.0/float(m))\n",
        "  \n",
        "  hc = []\n",
        "  z = []\n",
        "\n",
        "  #np.random.seed(2021)\n",
        "\n",
        "  X_train_for_sample = np.copy(X_train_array)\n",
        "  y_train_for_sample = np.copy(y_train_array)\n",
        "\n",
        "  for k in range(K):\n",
        "    \n",
        "    resampled_rows = np.random.choice(X_train_array.shape[0] , size=m , p=weight)\n",
        "    #print(resampled_rows)\n",
        "    X_train_for_sample = X_train_array[resampled_rows , :]\n",
        "    y_train_for_sample = y_train_array[resampled_rows , :]\n",
        "    #print(temp)\n",
        "    \n",
        "    w = logistic_regression(X_train_for_sample , y_train_for_sample , iterations , alpha , 0.5)\n",
        "\n",
        "    error = 0\n",
        "\n",
        "    # for j in range(m):\n",
        "    #   xj = X_train_for_sample.iloc[j]\n",
        "    #   hj = hypothesis(w,xj)\n",
        "    #   yj = y_train_for_sample.iloc[j]\n",
        "    #   if hj != yj:\n",
        "    #     error = error + weight[j]\n",
        "    \n",
        "    \n",
        "    X_train_for_sample_2 = np.concatenate((np.ones((m, 1)), X_train_for_sample), axis=1)\n",
        "    h = hypothesis(w , X_train_for_sample_2)\n",
        "    for j in range(m):\n",
        "      if h[j][0] < 0:\n",
        "        h[j][0] = -1\n",
        "      else:\n",
        "        h[j][0] = 1\n",
        "      yj = y_train_for_sample[j][0]\n",
        "      if h[j][0] != yj:\n",
        "        error = error + weight[j]\n",
        "\n",
        "    if error > 0.5:\n",
        "      print('bingo')\n",
        "      continue\n",
        "\n",
        "    # for j in range(m):\n",
        "    #   xj = X_train_for_sample.iloc[j]\n",
        "    #   hj = hypothesis(w,xj)\n",
        "    #   yj = y_train_for_sample.iloc[j]\n",
        "    #   if hj == yj:\n",
        "    #     weight[j] = weight[j] * float(error)/(1.0-float(error))\n",
        "\n",
        "    h = hypothesis(w , X_train_for_sample_2)\n",
        "    for j in range(m):\n",
        "      if h[j][0] < 0:\n",
        "        h[j][0] = -1\n",
        "      else:\n",
        "        h[j][0] = 1\n",
        "      yj = y_train_for_sample[j][0]\n",
        "      if h[j][0] == yj:\n",
        "        weight[j] = weight[j] * float(error)/(1.0-float(error))\n",
        "\n",
        "    weight = weight/np.sum(weight)\n",
        "\n",
        "    hc.append(w)\n",
        "    z.append(np.log2(  (1.0-float(error))   /   float(error)   ))\n",
        "\n",
        "  calculate_metrices(X_test_array , y_test_array , hc , z)\n",
        "  calculate_metrices(X_train_array , y_train_array , hc , z)\n",
        "  # def calculate_metrices(X_incoming , y_incoming):\n",
        "  #   X_test_final = np.concatenate((np.ones((X_test_array.shape[0], 1)), X_test_array), axis=1)\n",
        "  #   y_cap = weighted_majority(X_test_final , hc , z)\n",
        "  #   #print('accuracy_score : ' , accuracy_score(y_test_array , y_cap))\n",
        "\n",
        "  #   m2 = X_test_array.shape[0]\n",
        "  #   match = 0\n",
        "  #   for i in range(m2):\n",
        "  #     if y_cap[i] < 0:\n",
        "  #       y_cap[i] = -1\n",
        "  #     else:\n",
        "  #       y_cap[i] = 1\n",
        "\n",
        "  #     original_label = y_test_array[i][0]\n",
        "  #     if y_cap[i] == original_label:\n",
        "  #       match += 1\n",
        "    \n",
        "  #   CM = confusion_matrix(y_test_array , y_cap)\n",
        "  #   print(CM)\n",
        "  #   TP = CM[0][0]\n",
        "  #   FN = CM[0][1]\n",
        "  #   FP = CM[1][0]\n",
        "  #   TN = CM[1][1]\n",
        "  #   print('accuracy_score : ' , accuracy_score(y_test_array , y_cap))\n",
        "  #   print(float(match) / float(m2))\n",
        "  #   print('accuracy :' , (TP+TN)/(TP+TN+FP+FN))\n",
        "  #   sensitivity = TP/(TP+FN)\n",
        "  #   print('sensitivity :' , sensitivity)\n",
        "  #   specificity = TN/(TN+FP)\n",
        "  #   print('specificity :' , specificity)\n",
        "  #   precision = TP/(TP+FP)\n",
        "  #   print('precision :' , precision)\n",
        "  #   false_discovery_rate = 1 - precision\n",
        "  #   print('false_discovery_rate :' , false_discovery_rate)\n",
        "  #   f1_score = 2 * ((sensitivity*precision) / (sensitivity+precision))\n",
        "  #   print('f1_score :' , f1_score)"
      ],
      "metadata": {
        "id": "r88DlqqNZso1"
      },
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial_w = np.zeros(X_test.iloc[1].shape)\n",
        "# alpha = 0.05\n",
        "# iterations = 10\n",
        "# logistic_regression(alpha,initial_w,iterations)\n",
        "#initial_w = np.zeros(X_test.iloc[1].shape)\n",
        "alpha = 0.2\n",
        "iterations = 100\n",
        "AdaBoost(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1eFJZwLKRMo",
        "outputId": "7933c921-2a31-4e4e-b76b-95c5a5be7d08"
      },
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.7977288857345636\n",
            "Accuracy:  0.8005677785663591\n",
            "Accuracy:  0.7977288857345636\n",
            "Accuracy:  0.7856635911994322\n",
            "Accuracy:  0.78708303761533\n",
            "[[951  90]\n",
            " [188 180]]\n",
            "accuracy_score :  0.8026969481902059\n",
            "0.8026969481902059\n",
            "accuracy : 0.8026969481902059\n",
            "[[3725  408]\n",
            " [ 720  781]]\n",
            "accuracy_score :  0.7997870074547391\n",
            "0.7997870074547391\n",
            "accuracy : 0.7997870074547391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = np.array([1 , 2 , 3 , 4])\n",
        "# theta = np.zeros(x.shape)\n",
        "# theta[3] = 4\n",
        "# print(x-theta)\n",
        "a = round(0.2)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5GU7w8bUm_7",
        "outputId": "e8cd03be-2ef0-4384-e186-8a82d245d268"
      },
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 375
        }
      ]
    }
  ]
}