{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1605020.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 563,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC1U04CHb9fh",
        "outputId": "10d0d43a-3929-44bc-93d9-fbf4467c7f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/MyDrive/Adaboost/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xdFnWSQcv8h",
        "outputId": "2fcd15b9-bb32-466a-e244-715cf0b5136d"
      },
      "execution_count": 564,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/MyDrive/Adaboost/'\n",
            "/content/gdrive/MyDrive/Adaboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scikit-learn pandas matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAc4DO7cG0I",
        "outputId": "70901e19-2605-423e-df74-3fe15823ff1f"
      },
      "execution_count": 565,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "odlbMuIWcOph"
      },
      "execution_count": 566,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import minmax_scale, StandardScaler, MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "np.random.seed(16450)"
      ],
      "metadata": {
        "id": "GMz5GFS5cMXD"
      },
      "execution_count": 567,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(dataset):\n",
        "  for i in dataset.columns:\n",
        "    if(dataset[i].nunique()>2 and dataset[i].dtype != 'object'):\n",
        "      dataset[i] = (dataset[i]-dataset[i].min())/(dataset[i].max()-dataset[i].min())\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "45Qv60xEsBg8"
      },
      "execution_count": 568,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label = the name of prediction column\n",
        "label = ''"
      ],
      "metadata": {
        "id": "Ek_O2MUrs8VL"
      },
      "execution_count": 569,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess1():\n",
        "  dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "  dataset.drop(['customerID'] , inplace=True , axis='columns')\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    #print(i, dataset[i].nunique())\n",
        "    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):\n",
        "      #print(dataset[i].unique()[0], dataset[i].unique()[1])\n",
        "      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})\n",
        "\n",
        "  dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')\n",
        "\n",
        "  # mean = dataset['TotalCharges'].mean()\n",
        "  # dataset['TotalCharges'] = dataset['TotalCharges'].replace([np.nan],mean)\n",
        "  arr = []\n",
        "  for i in range(len(dataset['tenure'])):\n",
        "    if(dataset.loc[i , 'tenure'] == 0):\n",
        "      imagined_tenure = random.random()\n",
        "      # print(imagined_tenure)\n",
        "      # print(dataset.loc[i , ['MonthlyCharges']])\n",
        "      the_loc = dataset.loc[i , ['MonthlyCharges']]\n",
        "      # print(the_loc)\n",
        "      dataset.at[i , ['TotalCharges']] = float(the_loc * imagined_tenure)\n",
        "      # print(dataset.loc[i , ['TotalCharges']])\n",
        "      arr.append(i)\n",
        "  #dataset.info()\n",
        "  # for i in dataset['TotalCharges']:\n",
        "  #   if(np.isnan(i)):\n",
        "  #     i.replace(mean)\n",
        "\n",
        "  dataset['Churn'] = dataset['Churn'].map({1: 1, 0: -1})\n",
        "  print(dataset['Churn'].unique())\n",
        "\n",
        "\n",
        "  dataset = pd.get_dummies(dataset)\n",
        "  #print(dataset.head())\n",
        "\n",
        "  global label\n",
        "  label = 'Churn'\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#dataset = normalize(dataset)\n",
        "\n",
        "\n",
        "\n",
        "#dataset = dataset[1000:3050]\n",
        "# dataset.head(100)\n",
        "\n",
        "#dataset['TotalCharges'].count()\n",
        "\n",
        "#pd.Index(dataset['Churn']).value_counts()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QzSdN8Hr9-Ta"
      },
      "execution_count": 570,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.at[0, ['tenure']] = 0.2\n",
        "# dataset.loc[0 , ['tenure']]\n",
        "# arr = []\n",
        "# for i in range(len(dataset['tenure'])):\n",
        "#   if(dataset.loc[i , 'tenure'] == 0):\n",
        "#     imagined_tenure = random.random()\n",
        "#     dataset.at[i , ['TotalCharges']] = dataset.loc[i , ['MonthlyCharges']] * imagined_tenure\n",
        "#     arr.append(i)\n",
        "# dataset.loc[488 ,['TotalCharges']]\n"
      ],
      "metadata": {
        "id": "tbJaa_fI0ZVE"
      },
      "execution_count": 571,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess2(file_name):\n",
        "  dataset = pd.read_csv(file_name)\n",
        "\n",
        "  global label\n",
        "\n",
        "  label = ' Income'\n",
        "\n",
        "  # print(dataset.iloc[27])\n",
        "\n",
        "  # print(dataset[' Income'].head(100))\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    max_frequent_value = dataset[i].value_counts().idxmax()\n",
        "    dataset[i] = dataset[i].replace(' ?', max_frequent_value)\n",
        "\n",
        "  # print(dataset.iloc[27])\n",
        "  # print(dataset[' Income'].head(100))\n",
        "\n",
        "  if(file_name == 'adult_test.csv'):\n",
        "    dataset[' Income'] = dataset[' Income'].map({' <=50K.': (-1), ' >50K.': (1)})\n",
        "\n",
        "  else:\n",
        "    dataset[' Income'] = dataset[' Income'].map({' <=50K': (-1), ' >50K': (1)})\n",
        "\n",
        "  # print(dataset[' Income'].head(100))\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    #print(i, dataset[i].nunique())\n",
        "    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):\n",
        "      #print(dataset[i].unique()[0], dataset[i].unique()[1])\n",
        "      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # for i in dataset.columns:\n",
        "  #   if(dataset[i].nunique() == 2):\n",
        "  #     print(i)\n",
        "  #     print(dataset[i].unique())\n",
        "  \n",
        "  \n",
        "\n",
        "  #dataset.info()\n",
        "\n",
        "  dataset = pd.get_dummies(dataset, columns=onehot)\n",
        "\n",
        "  # for col in dataset.columns:\n",
        "  #   for j in dataset[col]:\n",
        "  #     if np.isnan(j) :\n",
        "  #       print(j)\n",
        "\n",
        "\n",
        "  # print(dataset.count().unique())\n",
        "\n",
        "  \n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mDpIbSYmd8-7"
      },
      "execution_count": 572,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess3():\n",
        "  global label\n",
        "  label = 'Class'\n",
        "\n",
        "  dataset = pd.read_csv('creditcard.csv')\n",
        "\n",
        "  dataset['Class'] = dataset['Class'].map({1: 1, 0: -1})\n",
        "  # print(dataset.isnull().sum())\n",
        "\n",
        "  df = dataset[dataset['Class'] == 1]\n",
        "\n",
        "  class_1_index = df.index\n",
        "\n",
        "  dataset.drop(class_1_index)\n",
        "\n",
        "  print(df.shape)\n",
        "\n",
        "  sampled = dataset.sample(n=500)\n",
        "\n",
        "  print(sampled.shape)\n",
        "\n",
        "  df2 = pd.concat([df, sampled])\n",
        "\n",
        "  df2 = shuffle(df2)\n",
        "\n",
        "  # print(df)\n",
        "\n",
        "  return df2\n",
        "\n",
        "dataset = preprocess3()\n",
        "\n",
        "print(dataset.head(100))\n",
        "\n",
        "print(dataset.tail(100))\n",
        "\n",
        "dataset.shape\n",
        "\n",
        "print(np.any(np.isnan(dataset.to_numpy())))\n",
        "\n",
        "# dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iHU_XpQddRT",
        "outputId": "cda6d2c3-515e-4650-cd9e-8b8965814ac3"
      },
      "execution_count": 573,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(492, 31)\n",
            "(500, 31)\n",
            "            Time        V1        V2  ...       V28  Amount  Class\n",
            "108513   70943.0 -1.417376  1.574589  ...  0.094340    4.00     -1\n",
            "189878  128595.0 -5.313774  2.664274  ...  0.362907    1.00      1\n",
            "82953    59624.0 -0.805676  1.392241  ...  0.025492   15.28     -1\n",
            "249936  154644.0  2.109507 -2.074296  ... -0.027467   18.00     -1\n",
            "6820      8614.0 -2.169929  3.639654  ...  0.298954    1.00      1\n",
            "...          ...       ...       ...  ...       ...     ...    ...\n",
            "112577   72710.0  1.193380  0.283391  ...  0.015044    1.98     -1\n",
            "3036      2597.0  1.239775  0.338243  ...  0.024439    0.89     -1\n",
            "219892  141925.0  0.120301  1.974141  ...  0.065039    0.76      1\n",
            "189821  128573.0 -1.782616  0.752994  ... -0.017079    6.09     -1\n",
            "20072    30765.0  0.034085 -0.800402  ...  0.270615  150.23     -1\n",
            "\n",
            "[100 rows x 31 columns]\n",
            "            Time         V1        V2  ...       V28  Amount  Class\n",
            "245610  152829.0   2.059980 -0.844971  ... -0.022223   44.00     -1\n",
            "192382  129668.0   0.753356  2.284988  ... -0.047174    2.00      1\n",
            "12108    21046.0 -16.917468  9.669900  ... -1.421243    1.00      1\n",
            "128479   78725.0  -4.312479  1.886476  ...  0.001454   60.00      1\n",
            "77470    57062.0  -1.188664 -0.612034  ... -0.018679    0.00     -1\n",
            "...          ...        ...       ...  ...       ...     ...    ...\n",
            "179986  124329.0  -0.207069  0.696625  ...  0.149354    2.58     -1\n",
            "123270   76867.0   1.082566  1.094862  ...  0.112337    1.00      1\n",
            "164441  116718.0   1.885207  0.465460  ... -0.048819   15.16     -1\n",
            "130690   79409.0   1.141512  0.031354  ...  0.006199   10.78     -1\n",
            "6480      7751.0   0.150552  0.506456  ...  0.171011    6.73     -1\n",
            "\n",
            "[100 rows x 31 columns]\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = preprocess1()\n",
        "# data_train = preprocess2('adult.csv')\n",
        "# data_test = preprocess2('adult_test.csv')\n",
        "\n",
        "dataset = preprocess3()\n",
        "\n",
        "# missing_cols = set( data_train.columns ) - set( data_test.columns )\n",
        "# # Add a missing column in test set with default value equal to 0\n",
        "# for c in missing_cols:\n",
        "#   data_test[c] = 0\n",
        "# # Ensure the order of column in the test set is in the same order than in train set\n",
        "# data_test = data_test[data_train.columns]\n",
        "  \n",
        "\n",
        "\n",
        "# x_train = data_train.drop([label], axis='columns')\n",
        "# y_train = data_train[label]\n",
        "\n",
        "\n",
        "\n",
        "# x_test = data_test.drop([label], axis='columns')\n",
        "# y_test = data_test[label]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = dataset.drop([label], axis='columns')\n",
        "y = dataset[label]\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "# # print(arr)\n",
        "\n",
        "# # print('nan paisi data te ',np.any(np.isnan(arr)))\n",
        "\n",
        "\n",
        "x_train = normalize(x_train)\n",
        "x_test = normalize(x_test)\n",
        "\n",
        "# print(x_train)\n",
        "# print(x_test)\n",
        "\n",
        "# arr = x_test.to_numpy()\n",
        "\n",
        "# # print('nan paisi data te ',np.any(np.isnan(arr)))\n",
        "\n",
        "# x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJDsVSaUdXju",
        "outputId": "58b7f646-e06e-4e4e-bf41-e5c98dc46f77"
      },
      "execution_count": 574,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(492, 31)\n",
            "(500, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = dataset.drop(['Churn'], axis='columns')\n",
        "\n",
        "# y = dataset['Churn']\n",
        "\n",
        "# # x\n",
        "# # y\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# x_train = normalize(x_train)\n",
        "# x_test = normalize(x_test)\n",
        "\n",
        "x_train_matrix = x_train.values\n",
        "x_test_matrix = x_test.values\n",
        "y_train_matrix = y_train.values.reshape(-1,1)\n",
        "y_test_matrix = y_test.values.reshape(-1,1)\n",
        "\n",
        "# print('nan paisi x tr te ',np.any(np.isnan(x_train_matrix)))\n",
        "# print('nan paisi y tr te ',np.any(np.isnan(y_train_matrix)))\n",
        "# print('nan paisi X tst te ',np.any(np.isnan(x_test_matrix)))\n",
        "# print('nan paisi t tst te ',np.any(np.isnan(y_test_matrix)))\n",
        "\n",
        "\n",
        "print(type(x_train_matrix))\n",
        "x_test_matrix.shape\n"
      ],
      "metadata": {
        "id": "ABZkromHYXJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799d8bf3-d981-45b7-bfc8-339ab2a1884e"
      },
      "execution_count": 575,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(199, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 575
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterative approach, takes too long\n",
        "\n",
        "\n",
        "def cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha):\n",
        "    sumErrors = 0\n",
        "    for i in range(m):\n",
        "        xi = x_train.iloc[i]\n",
        "        hi = hypothesistanh(theta,xi)\n",
        "        yt = y_train.iloc[i]\n",
        "        hit = np.subtract((float)(hi),float(yt))\n",
        "        error = np.dot(hit,x_train.iloc[i][j])\n",
        "        error = np.dot(error,(1-hi*hi))\n",
        "        sumErrors += error\n",
        "    #m = len(y_test)\n",
        "    constant = float(alpha)/float(m)\n",
        "    j = constant * sumErrors\n",
        "    #print(\"SumErrors\", sumErrors)\n",
        "    return j\n",
        "\n",
        "def gradient_descenttanh(x_train,y_train,theta,m,alpha):\n",
        "    new_theta = []\n",
        "    for j in range(len(theta)):\n",
        "        new_theta_value = theta[j] - cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha)\n",
        "        new_theta.append(new_theta_value)\n",
        "    #print ('theta ', new_theta)\t\n",
        "    return new_theta\n",
        "\n",
        "\n",
        "def error_calc(x_train,y_train,theta):\n",
        "  total = 0\n",
        "  m = len(y_train)\n",
        "  for i in range(m):\n",
        "        xi = x_train.iloc[i]\n",
        "        hi = hypothesistanh(theta,xi)\n",
        "        yt = y_train.iloc[i]\n",
        "        hit = np.subtract((float)(hi),float(yt))\n",
        "        total += hit*hit\n",
        "\n",
        "  return total/float(m)\n",
        "\n",
        "def logistic_regression_tanh(x_train,y_train,alpha,theta,num_iters, threshold):\n",
        "    m = len(y_train)\n",
        "    for x in range(num_iters):\n",
        "        new_theta = gradient_descenttanh(x_train,y_train,theta,m,alpha)\n",
        "        theta = new_theta\n",
        "        total = error_calc(x_train,y_train,theta)\n",
        "        if(total < threshold):\n",
        "          break\n",
        "        #if x % 1000 == 0:\n",
        "            #cost_functiontanh(theta,m)\n",
        "        #print ('theta ', theta)\n",
        "        #print('total' , total)\t\n",
        "            #print ('cost is ', cost_functiontanh(theta,m))\n",
        "    score = 0\n",
        "    length = len(x_test)\n",
        "    for i in range(length):\n",
        "        prediction = hypothesistanh(theta,x_test.iloc[i])\n",
        "        \n",
        "        if prediction < 0:\n",
        "          prediction = -1\n",
        "        else:\n",
        "          prediction = 1\n",
        "        # if(prediction == -1):\n",
        "        #   print(\"prediction \", prediction)\n",
        "        answer = y_test.iloc[i]\n",
        "        if prediction == answer:\n",
        "            score += 1\n",
        "    my_score = float(score) / float(length)\n",
        "    print ('Your score with tanh: ', my_score)\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "1aEa7LRfbMa0"
      },
      "execution_count": 576,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Tanh(z):\n",
        "    th = np.tanh(z)\n",
        "    # if(np.isnan(th)):\n",
        "    #   if(z < 0):\n",
        "    #     return -1\n",
        "    #   else:\n",
        "    #     return 1\n",
        "\n",
        "    return th"
      ],
      "metadata": {
        "id": "yWgRALXCxR9a"
      },
      "execution_count": 577,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hypothesistanh(theta,x):\n",
        "    #print ('theta ', theta)\t\n",
        "    # z = 0\n",
        "    # for i in range(len(theta)):\n",
        "    #     xi = x[i]\n",
        "    #     z += np.dot(xi,theta[i].transpose())\n",
        "\n",
        "    z = np.dot(x,theta)\n",
        "    # z = x @ theta\n",
        "    # print('z.shape',z.shape)\n",
        "    return Tanh(z)"
      ],
      "metadata": {
        "id": "VY4Ije16m2hS"
      },
      "execution_count": 578,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_error(y , y_hat):\n",
        "  diff = y - y_hat\n",
        "  return np.mean(np.power(diff , 2))"
      ],
      "metadata": {
        "id": "TliEEACLYoGC"
      },
      "execution_count": 579,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, y_hat, lr):\n",
        "    m = X.shape[0]\n",
        "    #dw = (1/m) * ( X.T @ ((y - y_hat) * (1 - y_hat**2)) )\n",
        "    \n",
        "    diff = y - y_hat\n",
        "    derivative = 1 - np.power(y_hat , 2)\n",
        "    multi = diff * derivative\n",
        "    # print('multi: ', multi.shape)\n",
        "    # print('derivative: ', derivative.shape)\n",
        "    # print('diff: ', diff.shape)\n",
        "\n",
        "    dw = np.dot(X.T , multi)\n",
        "    # dw = X.T @ multi\n",
        "    # print('dw: ', dw.shape)\n",
        "    constant = (1/float(m)) * float(lr) \n",
        "    return constant * dw"
      ],
      "metadata": {
        "id": "WQ7L3RgdO4aT"
      },
      "execution_count": 580,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_reg(X, y, iterations, lr, threshold):\n",
        "  m,n = X.shape\n",
        "\n",
        "  w = np.zeros((n+1 , 1))\n",
        "  # print(w.shape)\n",
        "\n",
        "  X_train = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
        "\n",
        "\n",
        "  errors = []\n",
        "\n",
        "  for i in range(iterations):\n",
        "    y_hat = hypothesistanh(w , X_train)\n",
        "    # print('y_hat' , y_hat.shape)\n",
        "\n",
        "    decay = gradient_descent(X_train , y , y_hat , lr)\n",
        "\n",
        "    if(np.isnan(decay.any())):\n",
        "      print('decay')\n",
        "    # print('decay :' , decay.shape)\n",
        "\n",
        "    # print('w_before :' , w.shape)\n",
        "    w = w + decay\n",
        "    # print('w_after :' , w.shape)\n",
        "\n",
        "    error = mean_error(y , y_hat)\n",
        "    errors.append(error)\n",
        "\n",
        "    if error < threshold:\n",
        "      print('bingo', i)\n",
        "      break\n",
        "\n",
        "  # print(w.shape)\n",
        "  score = 0\n",
        "  length = x_test_matrix.shape[0]\n",
        "  X_test = np.concatenate((np.ones((length, 1)), x_test_matrix), axis=1)\n",
        "  #print(X_test)\n",
        "  # print('X_test.shape : ' , X_test.shape)\n",
        "  # print('w.shape : ' , w.shape)\n",
        "  print('nan paisi w te ',np.any(np.isnan(w)))\n",
        "  print('nan paisi X te ',np.any(np.isnan(X_test)))\n",
        "  prediction = hypothesistanh(w, X_test)\n",
        "  # for i in range(prediction.shape[0]):\n",
        "  #   if(np.isnan(prediction[i].any())):\n",
        "  #       print('decay')\n",
        "\n",
        "  print('nan paisi',np.any(np.isnan(prediction)))\n",
        "\n",
        "  # print(prediction)\n",
        "  # print('pred uniq ', np.unique(prediction, axis=0))\n",
        "  \n",
        "  for i in range(length):\n",
        "    #X_test[i] = X_test[i].reshape(1,-1)\n",
        "    #print(X_test[i].shape)\n",
        "    #prediction = hypothesistanh(w, X_test[i])\n",
        "    # print(prediction.shape)\n",
        "    if prediction[i][0] < 0:\n",
        "      prediction[i][0] = -1\n",
        "    else:\n",
        "      prediction[i][0] = 1\n",
        "    # if(prediction == -1):\n",
        "    #   print(\"prediction \", prediction)\n",
        "    answer = y_test_matrix[i][0]\n",
        "    if prediction[i][0] == answer:\n",
        "      score += 1\n",
        "\n",
        "  print('pred uniq ', np.unique(prediction, axis=0))\n",
        "  my_score = float(score) / float(length)\n",
        "  print ('Your score with tanh: ', my_score)\n",
        "  \n",
        "  #print(w)\n",
        "  return w\n"
      ],
      "metadata": {
        "id": "97DDWw-5T1Ct"
      },
      "execution_count": 581,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpha = 0.1\n",
        "# iterations = 100\n",
        "# threshold = 0.5\n",
        "# w = log_reg(x_train_matrix , y_train_matrix , iterations , alpha , threshold)"
      ],
      "metadata": {
        "id": "ZXJUZC7hcLw_"
      },
      "execution_count": 582,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.array([[1,2],[2,3]])\n",
        "# b = np.array([4,5])\n",
        "# a.shape\n",
        "# c = a @ b\n",
        "# np.concatenate((np.ones((x_train_matrix.shape[0], 1)),x_train_matrix), axis=1)"
      ],
      "metadata": {
        "id": "LfJw_c2TkkPA"
      },
      "execution_count": 583,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Jbgusx2n8Y4"
      },
      "execution_count": 583,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ada_boost(num_iter):\n",
        "  m = y_train_matrix.shape[0]\n",
        "\n",
        "  weight = np.empty(m)\n",
        "  weight.fill(1.0/float(m))\n",
        "  #print(weight)\n",
        "  \n",
        "  hypothesis_container = []\n",
        "  hypothesis_weight = []\n",
        "\n",
        "  \n",
        "\n",
        "  x_train_sampled = np.copy(x_train_matrix)\n",
        "  # x_train_sampled_2 = np.concatenate((np.ones((m, 1)), x_train_matrix), axis=1)\n",
        "  y_train_sampled = np.copy(y_train_matrix)\n",
        "\n",
        "  for i in range(num_iter):\n",
        "    \n",
        "    resampled_int = np.random.choice(x_train_matrix.shape[0], size=m, p=weight)\n",
        "    # print(resampled_int)\n",
        "    x_train_sampled = x_train_matrix[resampled_int , :]\n",
        "    y_train_sampled = y_train_matrix[resampled_int , :]\n",
        "    #print(temp)\n",
        "    #print(y_train_sampled)\n",
        "    \n",
        "    theta = log_reg(np.copy(x_train_sampled), y_train_sampled, iterations , alpha, 0)\n",
        "    #hypothesis_container.append(theta)\n",
        "    #print(theta.shape)\n",
        "    #print(theta)\n",
        "\n",
        "    error = 0\n",
        "\n",
        "    x_train_sampled_2 = np.concatenate((np.ones((m, 1)), x_train_sampled), axis=1)\n",
        "    #print(x_train_sampled_2)\n",
        "\n",
        "    h = hypothesistanh(theta , x_train_sampled_2)\n",
        "    #print('go4')\n",
        "    for j in range(m):\n",
        "      #xj = x_train_sampled.iloc[j]\n",
        "      #h = hypothesistanh(theta,xj)\n",
        "      if h[j][0] < 0:\n",
        "        h[j][0] = -1\n",
        "      else:\n",
        "        h[j][0] = 1\n",
        "      yj = y_train_sampled[j][0]\n",
        "      if h[j][0] != yj:\n",
        "        error = error + weight[j]\n",
        "\n",
        "    if error > 0.5:\n",
        "      #print(error)\n",
        "      continue\n",
        "\n",
        "    h = hypothesistanh(theta , x_train_sampled_2)\n",
        "    for j in range(m):\n",
        "      # xj = x_train_sampled.iloc[j]\n",
        "      # hj = hypothesistanh(theta,xj)\n",
        "      if h[j][0] < 0:\n",
        "        h[j][0] = -1\n",
        "      else:\n",
        "        h[j][0] = 1\n",
        "      yj = y_train_sampled[j][0]\n",
        "      if h[j][0] == yj:\n",
        "        weight[j] = weight[j] * float(error)/(1.0-float(error))\n",
        "\n",
        "    weight = weight/np.sum(weight)\n",
        "    #print(weight)\n",
        "\n",
        "    hypothesis_container.append(theta)\n",
        "    hypothesis_weight.append(np.log2(  (1.0-float(error))   /   float(error)   ))\n",
        "\n",
        "  \n",
        "  X_test = np.concatenate((np.ones((x_test_matrix.shape[0], 1)), x_test_matrix), axis=1)\n",
        "  #print('go')\n",
        "\n",
        "  h_cap = Weighted_majority(X_test , hypothesis_container , hypothesis_weight)\n",
        "  #print(h_cap)\n",
        "  #print('go2')\n",
        "\n",
        "  m2 = len(x_test)\n",
        "  score = 0\n",
        "  for i in range(m2):\n",
        "    if h_cap[i] < 0:\n",
        "      h_cap[i] = -1\n",
        "    else:\n",
        "      h_cap[i] = 1\n",
        "\n",
        "    answer = y_test_matrix[i][0]\n",
        "    if h_cap[i] == answer:\n",
        "      score += 1\n",
        "  # print(h_cap)\n",
        "  # print(np.unique(h_cap))\n",
        "  print(float(score) / float(m2))\n",
        "\n",
        "\n",
        "def Weighted_majority(x , hypothesis_container , hypothesis_weight):\n",
        "  m = len(x)\n",
        "  iter = len(hypothesis_weight)\n",
        "  #print(iter)\n",
        "  h_cap = np.zeros(m)\n",
        "  for k in range(iter):\n",
        "    h = hypothesistanh(hypothesis_container[k].reshape(-1, 1), x)\n",
        "    for i in range(m):\n",
        "      \n",
        "      mul = h[i][0]*hypothesis_weight[k]\n",
        "      h_cap[i] = h_cap[i] + mul\n",
        "\n",
        "  #print(h_cap)\n",
        "  return h_cap\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KP0a_9a6qma9"
      },
      "execution_count": 584,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJmYWIEC8ytc",
        "outputId": "13de7952-d1d1-477b-c243-47bdd7ebcd20"
      },
      "execution_count": 585,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(199, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 585
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_theta = np.zeros(x_test.iloc[1].shape)\n",
        "#x_test.iloc[1]\n",
        "alpha = 0.1\n",
        "iterations = 1000\n",
        "#logistic_regression_tanh(alpha,initial_theta,iterations)\n",
        "ada_boost(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F14TE4Kjohh4",
        "outputId": "d537390d-e6ff-4b19-cdee-e6bf0b594409"
      },
      "execution_count": 587,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "pred uniq  [[-1.]\n",
            " [ 1.]]\n",
            "Your score with tanh:  0.9346733668341709\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "pred uniq  [[-1.]\n",
            " [ 1.]]\n",
            "Your score with tanh:  0.9296482412060302\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "pred uniq  [[-1.]\n",
            " [ 1.]]\n",
            "Your score with tanh:  0.9547738693467337\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "pred uniq  [[-1.]\n",
            " [ 1.]]\n",
            "Your score with tanh:  0.9346733668341709\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "pred uniq  [[-1.]\n",
            " [ 1.]]\n",
            "Your score with tanh:  0.9095477386934674\n",
            "0.9346733668341709\n"
          ]
        }
      ]
    }
  ]
}