{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC1U04CHb9fh",
        "outputId": "16fbe7e4-dae3-4bbd-cfe4-d47e5f31ef18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xdFnWSQcv8h",
        "outputId": "8de16d89-41ce-4d25-a74e-d611f994a525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/MyDrive/Adaboost/'\n",
            "/content/gdrive/MyDrive/Adaboost\n"
          ]
        }
      ],
      "source": [
        "%cd gdrive/MyDrive/Adaboost/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAc4DO7cG0I",
        "outputId": "cbc6e56b-4e1b-4495-a647-f9a837074ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "odlbMuIWcOph"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10348/2043316767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "GMz5GFS5cMXD"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import minmax_scale, StandardScaler, MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "45Qv60xEsBg8"
      },
      "outputs": [],
      "source": [
        "def normalize(dataset):\n",
        "  for i in dataset.columns:\n",
        "    if(dataset[i].nunique()>2 and dataset[i].dtype != 'object'):\n",
        "      dataset[i] = (dataset[i]-dataset[i].min())/(dataset[i].max()-dataset[i].min())\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "Ek_O2MUrs8VL"
      },
      "outputs": [],
      "source": [
        "# label = the name of prediction column\n",
        "label = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "id": "QzSdN8Hr9-Ta"
      },
      "outputs": [],
      "source": [
        "def preprocess1():\n",
        "  dataset = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "  dataset.drop(['customerID'] , inplace=True , axis='columns')\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    #print(i, dataset[i].nunique())\n",
        "    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):\n",
        "      #print(dataset[i].unique()[0], dataset[i].unique()[1])\n",
        "      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})\n",
        "\n",
        "  dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')\n",
        "\n",
        "  # mean = dataset['TotalCharges'].mean()\n",
        "  # dataset['TotalCharges'] = dataset['TotalCharges'].replace([np.nan],mean)\n",
        "  arr = []\n",
        "  for i in range(len(dataset['tenure'])):\n",
        "    if(dataset.loc[i , 'tenure'] == 0):\n",
        "      imagined_tenure = random.random()\n",
        "      # print(imagined_tenure)\n",
        "      # print(dataset.loc[i , ['MonthlyCharges']])\n",
        "      the_loc = dataset.loc[i , ['MonthlyCharges']]\n",
        "      # print(the_loc)\n",
        "      dataset.at[i , ['TotalCharges']] = float(the_loc * imagined_tenure)\n",
        "      # print(dataset.loc[i , ['TotalCharges']])\n",
        "      arr.append(i)\n",
        "  #dataset.info()\n",
        "  # for i in dataset['TotalCharges']:\n",
        "  #   if(np.isnan(i)):\n",
        "  #     i.replace(mean)\n",
        "\n",
        "  dataset['Churn'] = dataset['Churn'].map({1: 1, 0: -1})\n",
        "  print(dataset['Churn'].unique())\n",
        "\n",
        "\n",
        "  dataset = pd.get_dummies(dataset)\n",
        "  #print(dataset.head())\n",
        "\n",
        "  global label\n",
        "  label = 'Churn'\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#dataset = normalize(dataset)\n",
        "\n",
        "\n",
        "\n",
        "#dataset = dataset[1000:3050]\n",
        "# dataset.head(100)\n",
        "\n",
        "#dataset['TotalCharges'].count()\n",
        "\n",
        "#pd.Index(dataset['Churn']).value_counts()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "tbJaa_fI0ZVE"
      },
      "outputs": [],
      "source": [
        "# dataset.at[0, ['tenure']] = 0.2\n",
        "# dataset.loc[0 , ['tenure']]\n",
        "# arr = []\n",
        "# for i in range(len(dataset['tenure'])):\n",
        "#   if(dataset.loc[i , 'tenure'] == 0):\n",
        "#     imagined_tenure = random.random()\n",
        "#     dataset.at[i , ['TotalCharges']] = dataset.loc[i , ['MonthlyCharges']] * imagined_tenure\n",
        "#     arr.append(i)\n",
        "# dataset.loc[488 ,['TotalCharges']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDpIbSYmd8-7",
        "outputId": "3657ae42-338b-4b28-88de-9f5ba71e7bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train  (32561, 104)\n",
            "0     -1\n",
            "1     -1\n",
            "2     -1\n",
            "3     -1\n",
            "4     -1\n",
            "      ..\n",
            "995    1\n",
            "996   -1\n",
            "997    1\n",
            "998   -1\n",
            "999    1\n",
            "Name:  Income, Length: 1000, dtype: int64\n",
            "x_test  (16281, 104)\n"
          ]
        }
      ],
      "source": [
        "def preprocess2(file_name):\n",
        "  dataset = pd.read_csv(file_name)\n",
        "\n",
        "  global label\n",
        "\n",
        "  label = ' Income'\n",
        "\n",
        "  # print(dataset.iloc[27])\n",
        "\n",
        "  # print(dataset[' Income'].head(100))\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    max_frequent_value = dataset[i].value_counts().idxmax()\n",
        "    dataset[i] = dataset[i].replace(' ?', max_frequent_value)\n",
        "\n",
        "  # print(dataset.iloc[27])\n",
        "  # print(dataset[' Income'].head(100))\n",
        "\n",
        "  if(file_name == 'adult_test.csv'):\n",
        "    dataset[' Income'] = dataset[' Income'].map({' <=50K.': (-1), ' >50K.': (1)})\n",
        "\n",
        "  else:\n",
        "    dataset[' Income'] = dataset[' Income'].map({' <=50K': (-1), ' >50K': (1)})\n",
        "\n",
        "  # print(dataset[' Income'].head(100))\n",
        "\n",
        "  for i in dataset.columns:\n",
        "    #print(i, dataset[i].nunique())\n",
        "    if(dataset[i].nunique() == 2 and dataset[i].dtype == 'object'):\n",
        "      #print(dataset[i].unique()[0], dataset[i].unique()[1])\n",
        "      dataset[i] = dataset[i].map({dataset[i].unique()[0] : 0, dataset[i].unique()[1] : 1})\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # for i in dataset.columns:\n",
        "  #   if(dataset[i].nunique() == 2):\n",
        "  #     print(i)\n",
        "  #     print(dataset[i].unique())\n",
        "  \n",
        "  \n",
        "\n",
        "  #dataset.info()\n",
        "\n",
        "  dataset = pd.get_dummies(dataset, columns=onehot)\n",
        "\n",
        "  # for col in dataset.columns:\n",
        "  #   for j in dataset[col]:\n",
        "  #     if np.isnan(j) :\n",
        "  #       print(j)\n",
        "\n",
        "\n",
        "  # print(dataset.count().unique())\n",
        "\n",
        "  \n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "# dataset = preprocess1()\n",
        "data_train = preprocess2('adult.csv')\n",
        "data_test = preprocess2('adult_test.csv')\n",
        "\n",
        "missing_cols = set( data_train.columns ) - set( data_test.columns )\n",
        "# Add a missing column in test set with default value equal to 0\n",
        "for c in missing_cols:\n",
        "  data_test[c] = 0\n",
        "# Ensure the order of column in the test set is in the same order than in train set\n",
        "data_test = data_test[data_train.columns]\n",
        "  \n",
        "\n",
        "\n",
        "x_train = data_train.drop([label], axis='columns')\n",
        "y_train = data_train[label]\n",
        "\n",
        "print('x_train ',x_train.shape)\n",
        "\n",
        "print(y_train.head(1000))\n",
        "\n",
        "\n",
        "x_test = data_test.drop([label], axis='columns')\n",
        "y_test = data_test[label]\n",
        "\n",
        "print('x_test ',x_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x = dataset.drop([label], axis='columns')\n",
        "# y = dataset[label]\n",
        "\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "\n",
        "arr = x_test.to_numpy()\n",
        "\n",
        "# # print(arr)\n",
        "\n",
        "# # print('nan paisi data te ',np.any(np.isnan(arr)))\n",
        "\n",
        "\n",
        "x_train = normalize(x_train)\n",
        "x_test = normalize(x_test)\n",
        "\n",
        "# arr = x_test.to_numpy()\n",
        "\n",
        "# # print('nan paisi data te ',np.any(np.isnan(arr)))\n",
        "\n",
        "# x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABZkromHYXJX",
        "outputId": "5e5ffdc1-7068-4c33-b4e0-6a07e90d12c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(16281, 104)"
            ]
          },
          "execution_count": 385,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# x = dataset.drop(['Churn'], axis='columns')\n",
        "\n",
        "# y = dataset['Churn']\n",
        "\n",
        "# # x\n",
        "# # y\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# x_train = normalize(x_train)\n",
        "# x_test = normalize(x_test)\n",
        "\n",
        "x_train_matrix = x_train.values\n",
        "x_test_matrix = x_test.values\n",
        "y_train_matrix = y_train.values.reshape(-1,1)\n",
        "y_test_matrix = y_test.values.reshape(-1,1)\n",
        "\n",
        "# print('nan paisi x tr te ',np.any(np.isnan(x_train_matrix)))\n",
        "# print('nan paisi y tr te ',np.any(np.isnan(y_train_matrix)))\n",
        "# print('nan paisi X tst te ',np.any(np.isnan(x_test_matrix)))\n",
        "# print('nan paisi t tst te ',np.any(np.isnan(y_test_matrix)))\n",
        "\n",
        "\n",
        "print(type(x_train_matrix))\n",
        "x_test_matrix.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "1aEa7LRfbMa0"
      },
      "outputs": [],
      "source": [
        "# iterative approach, takes too long\n",
        "\n",
        "\n",
        "def cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha):\n",
        "    sumErrors = 0\n",
        "    for i in range(m):\n",
        "        xi = x_train.iloc[i]\n",
        "        hi = hypothesistanh(theta,xi)\n",
        "        yt = y_train.iloc[i]\n",
        "        hit = np.subtract((float)(hi),float(yt))\n",
        "        error = np.dot(hit,x_train.iloc[i][j])\n",
        "        error = np.dot(error,(1-hi*hi))\n",
        "        sumErrors += error\n",
        "    #m = len(y_test)\n",
        "    constant = float(alpha)/float(m)\n",
        "    j = constant * sumErrors\n",
        "    #print(\"SumErrors\", sumErrors)\n",
        "    return j\n",
        "\n",
        "def gradient_descenttanh(x_train,y_train,theta,m,alpha):\n",
        "    new_theta = []\n",
        "    for j in range(len(theta)):\n",
        "        new_theta_value = theta[j] - cost_function_derivativetanh(x_train,y_train,theta,j,m,alpha)\n",
        "        new_theta.append(new_theta_value)\n",
        "    #print ('theta ', new_theta)\t\n",
        "    return new_theta\n",
        "\n",
        "\n",
        "def error_calc(x_train,y_train,theta):\n",
        "  total = 0\n",
        "  m = len(y_train)\n",
        "  for i in range(m):\n",
        "        xi = x_train.iloc[i]\n",
        "        hi = hypothesistanh(theta,xi)\n",
        "        yt = y_train.iloc[i]\n",
        "        hit = np.subtract((float)(hi),float(yt))\n",
        "        total += hit*hit\n",
        "\n",
        "  return total/float(m)\n",
        "\n",
        "def logistic_regression_tanh(x_train,y_train,alpha,theta,num_iters, threshold):\n",
        "    m = len(y_train)\n",
        "    for x in range(num_iters):\n",
        "        new_theta = gradient_descenttanh(x_train,y_train,theta,m,alpha)\n",
        "        theta = new_theta\n",
        "        total = error_calc(x_train,y_train,theta)\n",
        "        if(total < threshold):\n",
        "          break\n",
        "        #if x % 1000 == 0:\n",
        "            #cost_functiontanh(theta,m)\n",
        "        #print ('theta ', theta)\n",
        "        #print('total' , total)\t\n",
        "            #print ('cost is ', cost_functiontanh(theta,m))\n",
        "    score = 0\n",
        "    length = len(x_test)\n",
        "    for i in range(length):\n",
        "        prediction = hypothesistanh(theta,x_test.iloc[i])\n",
        "        \n",
        "        if prediction < 0:\n",
        "          prediction = -1\n",
        "        else:\n",
        "          prediction = 1\n",
        "        # if(prediction == -1):\n",
        "        #   print(\"prediction \", prediction)\n",
        "        answer = y_test.iloc[i]\n",
        "        if prediction == answer:\n",
        "            score += 1\n",
        "    my_score = float(score) / float(length)\n",
        "    print ('Your score with tanh: ', my_score)\n",
        "\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "yWgRALXCxR9a"
      },
      "outputs": [],
      "source": [
        "def Tanh(z):\n",
        "    th = np.tanh(z)\n",
        "    # if(np.isnan(th)):\n",
        "    #   if(z < 0):\n",
        "    #     return -1\n",
        "    #   else:\n",
        "    #     return 1\n",
        "\n",
        "    return th"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "id": "VY4Ije16m2hS"
      },
      "outputs": [],
      "source": [
        "def hypothesistanh(theta,x):\n",
        "    #print ('theta ', theta)\t\n",
        "    # z = 0\n",
        "    # for i in range(len(theta)):\n",
        "    #     xi = x[i]\n",
        "    #     z += np.dot(xi,theta[i].transpose())\n",
        "\n",
        "    z = np.dot(x,theta)\n",
        "    # z = x @ theta\n",
        "    # print('z.shape',z.shape)\n",
        "    return Tanh(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "id": "TliEEACLYoGC"
      },
      "outputs": [],
      "source": [
        "def mean_error(y , y_hat):\n",
        "  diff = y - y_hat\n",
        "  return np.mean(np.power(diff , 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "WQ7L3RgdO4aT"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, y_hat, lr):\n",
        "    m = X.shape[0]\n",
        "    #dw = (1/m) * ( X.T @ ((y - y_hat) * (1 - y_hat**2)) )\n",
        "    \n",
        "    diff = y - y_hat\n",
        "    derivative = 1 - np.power(y_hat , 2)\n",
        "    multi = diff * derivative\n",
        "    # print('multi: ', multi.shape)\n",
        "    # print('derivative: ', derivative.shape)\n",
        "    # print('diff: ', diff.shape)\n",
        "\n",
        "    dw = np.dot(X.T , multi)\n",
        "    # dw = X.T @ multi\n",
        "    # print('dw: ', dw.shape)\n",
        "    constant = (1/float(m)) * float(lr) \n",
        "    return constant * dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "id": "97DDWw-5T1Ct"
      },
      "outputs": [],
      "source": [
        "def log_reg(X, y, iterations, lr, threshold):\n",
        "  m,n = X.shape\n",
        "\n",
        "  w = np.zeros((n+1 , 1))\n",
        "  # print(w.shape)\n",
        "\n",
        "  X_train = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
        "\n",
        "\n",
        "  errors = []\n",
        "\n",
        "  for i in range(iterations):\n",
        "    y_hat = hypothesistanh(w , X_train)\n",
        "    # print('y_hat' , y_hat.shape)\n",
        "\n",
        "    decay = gradient_descent(X_train , y , y_hat , lr)\n",
        "\n",
        "    if(np.isnan(decay.any())):\n",
        "      print('decay')\n",
        "    # print('decay :' , decay.shape)\n",
        "\n",
        "    # print('w_before :' , w.shape)\n",
        "    w = w + decay\n",
        "    # print('w_after :' , w.shape)\n",
        "\n",
        "    error = mean_error(y , y_hat)\n",
        "    errors.append(error)\n",
        "\n",
        "    if error < threshold:\n",
        "      print('bingo', i)\n",
        "      break\n",
        "\n",
        "  # print(w.shape)\n",
        "  score = 0\n",
        "  length = x_test_matrix.shape[0]\n",
        "  X_test = np.concatenate((np.ones((length, 1)), x_test_matrix), axis=1)\n",
        "  #print(X_test)\n",
        "  # print('X_test.shape : ' , X_test.shape)\n",
        "  # print('w.shape : ' , w.shape)\n",
        "  print('nan paisi w te ',np.any(np.isnan(w)))\n",
        "  print('nan paisi X te ',np.any(np.isnan(X_test)))\n",
        "  prediction = hypothesistanh(w, X_test)\n",
        "  # for i in range(prediction.shape[0]):\n",
        "  #   if(np.isnan(prediction[i].any())):\n",
        "  #       print('decay')\n",
        "\n",
        "  print('nan paisi',np.any(np.isnan(prediction)))\n",
        "\n",
        "  # print(prediction)\n",
        "  #print('pred uniq ', np.unique(prediction, axis=0))\n",
        "  \n",
        "  for i in range(length):\n",
        "    #X_test[i] = X_test[i].reshape(1,-1)\n",
        "    #print(X_test[i].shape)\n",
        "    #prediction = hypothesistanh(w, X_test[i])\n",
        "    # print(prediction.shape)\n",
        "    if prediction[i][0] < 0:\n",
        "      prediction[i][0] = -1\n",
        "    else:\n",
        "      prediction[i][0] = 1\n",
        "    # if(prediction == -1):\n",
        "    #   print(\"prediction \", prediction)\n",
        "    answer = y_test_matrix[i][0]\n",
        "    if prediction[i][0] == answer:\n",
        "      score += 1\n",
        "\n",
        "  #print('pred uniq ', np.unique(prediction, axis=0))\n",
        "  my_score = float(score) / float(length)\n",
        "  print ('Your score with tanh: ', my_score)\n",
        "  \n",
        "  #print(w)\n",
        "  return w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "id": "ZXJUZC7hcLw_"
      },
      "outputs": [],
      "source": [
        "# alpha = 0.1\n",
        "# iterations = 100\n",
        "# threshold = 0.5\n",
        "# w = log_reg(x_train_matrix , y_train_matrix , iterations , alpha , threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "LfJw_c2TkkPA"
      },
      "outputs": [],
      "source": [
        "# a = np.array([[1,2],[2,3]])\n",
        "# b = np.array([4,5])\n",
        "# a.shape\n",
        "# c = a @ b\n",
        "# np.concatenate((np.ones((x_train_matrix.shape[0], 1)),x_train_matrix), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "0Jbgusx2n8Y4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "id": "KP0a_9a6qma9"
      },
      "outputs": [],
      "source": [
        "def ada_boost(num_iter):\n",
        "  m = y_train_matrix.shape[0]\n",
        "\n",
        "  weight = np.empty(m)\n",
        "  weight.fill(1.0/float(m))\n",
        "  #print(weight)\n",
        "  \n",
        "  hypothesis_container = []\n",
        "  hypothesis_weight = []\n",
        "\n",
        "  np.random.seed(5)\n",
        "\n",
        "  x_train_sampled = np.copy(x_train_matrix)\n",
        "  # x_train_sampled_2 = np.concatenate((np.ones((m, 1)), x_train_matrix), axis=1)\n",
        "  y_train_sampled = np.copy(y_train_matrix)\n",
        "\n",
        "  for i in range(num_iter):\n",
        "    \n",
        "    resampled_int = np.random.choice(x_train_matrix.shape[0], size=m, p=weight)\n",
        "    # print(resampled_int)\n",
        "    x_train_sampled = x_train_matrix[resampled_int , :]\n",
        "    y_train_sampled = y_train_matrix[resampled_int , :]\n",
        "    #print(temp)\n",
        "    #print(y_train_sampled)\n",
        "    \n",
        "    theta = log_reg(np.copy(x_train_sampled), y_train_sampled, iterations , alpha, 0.5)\n",
        "    #hypothesis_container.append(theta)\n",
        "    #print(theta.shape)\n",
        "    #print(theta)\n",
        "\n",
        "    error = 0\n",
        "\n",
        "    x_train_sampled_2 = np.concatenate((np.ones((m, 1)), x_train_sampled), axis=1)\n",
        "    #print(x_train_sampled_2)\n",
        "\n",
        "    h = hypothesistanh(theta , x_train_sampled_2)\n",
        "    #print('go4')\n",
        "    for j in range(m):\n",
        "      #xj = x_train_sampled.iloc[j]\n",
        "      #h = hypothesistanh(theta,xj)\n",
        "      if h[j][0] < 0:\n",
        "        h[j][0] = -1\n",
        "      else:\n",
        "        h[j][0] = 1\n",
        "      yj = y_train_sampled[j][0]\n",
        "      if h[j][0] != yj:\n",
        "        error = error + weight[j]\n",
        "\n",
        "    if error > 0.5:\n",
        "      #print(error)\n",
        "      continue\n",
        "\n",
        "    h = hypothesistanh(theta , x_train_sampled_2)\n",
        "    for j in range(m):\n",
        "      # xj = x_train_sampled.iloc[j]\n",
        "      # hj = hypothesistanh(theta,xj)\n",
        "      if h[j][0] < 0:\n",
        "        h[j][0] = -1\n",
        "      else:\n",
        "        h[j][0] = 1\n",
        "      yj = y_train_sampled[j][0]\n",
        "      if h[j][0] == yj:\n",
        "        weight[j] = weight[j] * float(error)/(1.0-float(error))\n",
        "\n",
        "    weight = weight/np.sum(weight)\n",
        "    #print(weight)\n",
        "\n",
        "    hypothesis_container.append(theta)\n",
        "    hypothesis_weight.append(np.log2(  (1.0-float(error))   /   float(error)   ))\n",
        "\n",
        "  \n",
        "  X_test = np.concatenate((np.ones((x_test_matrix.shape[0], 1)), x_test_matrix), axis=1)\n",
        "  #print('go')\n",
        "\n",
        "  h_cap = Weighted_majority(X_test , hypothesis_container , hypothesis_weight)\n",
        "  #print(h_cap)\n",
        "  #print('go2')\n",
        "\n",
        "  m2 = len(x_test)\n",
        "  score = 0\n",
        "  for i in range(m2):\n",
        "    if h_cap[i] < 0:\n",
        "      h_cap[i] = -1\n",
        "    else:\n",
        "      h_cap[i] = 1\n",
        "\n",
        "    answer = y_test_matrix[i][0]\n",
        "    if h_cap[i] == answer:\n",
        "      score += 1\n",
        "  # print(h_cap)\n",
        "  # print(np.unique(h_cap))\n",
        "  print(float(score) / float(m2))\n",
        "\n",
        "\n",
        "def Weighted_majority(x , hypothesis_container , hypothesis_weight):\n",
        "  m = len(x)\n",
        "  iter = len(hypothesis_weight)\n",
        "  #print(iter)\n",
        "  h_cap = np.zeros(m)\n",
        "  for k in range(iter):\n",
        "    h = hypothesistanh(hypothesis_container[k].reshape(-1, 1), x)\n",
        "    for i in range(m):\n",
        "      \n",
        "      mul = h[i][0]*hypothesis_weight[k]\n",
        "      h_cap[i] = h_cap[i] + mul\n",
        "\n",
        "  #print(h_cap)\n",
        "  return h_cap\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJmYWIEC8ytc",
        "outputId": "bc0c115c-f6b4-4311-d444-5376f813b21c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16281, 104)"
            ]
          },
          "execution_count": 395,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F14TE4Kjohh4",
        "outputId": "9c1b0c5e-5a7f-48a5-80d3-406badfe946b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bingo 158\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "Your score with tanh:  0.8286346047540077\n",
            "bingo 132\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "Your score with tanh:  0.8264848596523555\n",
            "bingo 154\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "Your score with tanh:  0.8305386647011854\n",
            "bingo 108\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "Your score with tanh:  0.8265462809409742\n",
            "bingo 96\n",
            "nan paisi w te  False\n",
            "nan paisi X te  False\n",
            "nan paisi False\n",
            "Your score with tanh:  0.8217554204287206\n",
            "0.8277132854247282\n"
          ]
        }
      ],
      "source": [
        "initial_theta = np.zeros(x_test.iloc[1].shape)\n",
        "#x_test.iloc[1]\n",
        "alpha = 0.1\n",
        "iterations = 1000\n",
        "#logistic_regression_tanh(alpha,initial_theta,iterations)\n",
        "ada_boost(5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1605020.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
